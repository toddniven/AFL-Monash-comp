{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "pd.set_option(\"display.max_columns\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Proxy settings when using cntlm\n",
    "http_proxy  = \"http://localhost:3128\"\n",
    "https_proxy = \"https://localhost:3128\"\n",
    "\n",
    "proxyDict = { \n",
    "              \"http\"  : http_proxy, \n",
    "              \"https\" : https_proxy, \n",
    "            }\n",
    "proxyDict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'Richmond': 'richmond',\n",
    "    'West Coast': 'westcoast',\n",
    "    'Sydney': 'swans',\n",
    "    'Adelaide': 'adelaide',\n",
    "    'Geelong': 'geelong',\n",
    "    'Greater Western Sydney': 'gws',\n",
    "    'Melbourne': 'melbourne',\n",
    "    'Port Adelaide': 'padelaide',\n",
    "    'Collingwood': 'collingwood',\n",
    "    'Hawthorn': 'hawthorn',\n",
    "    'Essendon': 'essendon',\n",
    "    'Western Bulldogs': 'bullldogs',\n",
    "    'St Kilda': 'stkilda',\n",
    "    'North Melbourne': 'kangaroos',\n",
    "    'Kangaroos' : 'kangaroos',\n",
    "    'Fremantle': 'fremantle',\n",
    "    'Brisbane Lions': 'brisbanel',\n",
    "    'Gold Coast': 'goldcoast',\n",
    "    'Carlton': 'carlton'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Richmond 1\n",
      "Richmond 2\n",
      "Richmond 3\n",
      "Richmond 4\n",
      "Richmond 5\n",
      "Richmond 6\n",
      "Richmond 7\n",
      "Richmond 8\n",
      "Richmond 9\n",
      "Richmond 10\n",
      "Richmond 11\n",
      "Richmond 12\n",
      "Richmond 13\n",
      "Richmond 14\n",
      "Richmond 15\n",
      "West Coast 1\n",
      "West Coast 2\n",
      "West Coast 3\n",
      "West Coast 4\n",
      "West Coast 5\n",
      "West Coast 6\n",
      "West Coast 7\n",
      "West Coast 8\n",
      "West Coast 9\n",
      "West Coast 10\n",
      "West Coast 11\n",
      "West Coast 12\n",
      "West Coast 13\n",
      "West Coast 14\n",
      "West Coast 15\n",
      "Sydney 1\n",
      "Sydney 2\n",
      "Sydney 3\n",
      "Sydney 4\n",
      "Sydney 5\n",
      "Sydney 6\n",
      "Sydney 7\n",
      "Sydney 8\n",
      "Sydney 9\n",
      "Sydney 10\n",
      "Sydney 11\n",
      "Sydney 12\n",
      "Sydney 13\n",
      "Sydney 14\n",
      "Sydney 15\n",
      "Adelaide 1\n",
      "Adelaide 2\n",
      "Adelaide 3\n",
      "Adelaide 4\n",
      "Adelaide 5\n",
      "Adelaide 6\n",
      "Adelaide 7\n",
      "Adelaide 8\n",
      "Adelaide 9\n",
      "Adelaide 10\n",
      "Adelaide 11\n",
      "Adelaide 12\n",
      "Adelaide 13\n",
      "Adelaide 14\n",
      "Adelaide 15\n",
      "Geelong 1\n",
      "Geelong 2\n",
      "Geelong 3\n",
      "Geelong 4\n",
      "Geelong 5\n",
      "Geelong 6\n",
      "Geelong 7\n",
      "Geelong 8\n",
      "Geelong 9\n",
      "Geelong 10\n",
      "Geelong 11\n",
      "Geelong 12\n",
      "Geelong 13\n",
      "Geelong 14\n",
      "Geelong 15\n",
      "Greater Western Sydney 1\n",
      "Greater Western Sydney 2\n",
      "Greater Western Sydney 3\n",
      "Greater Western Sydney 4\n",
      "Greater Western Sydney 5\n",
      "Greater Western Sydney 6\n",
      "Greater Western Sydney 7\n",
      "Greater Western Sydney 8\n",
      "Greater Western Sydney 9\n",
      "Greater Western Sydney 10\n",
      "Greater Western Sydney 11\n",
      "Greater Western Sydney 12\n",
      "Greater Western Sydney 13\n",
      "Greater Western Sydney 14\n",
      "Greater Western Sydney 15\n",
      "Melbourne 1\n",
      "Melbourne 2\n",
      "Melbourne 3\n",
      "Melbourne 4\n",
      "Melbourne 5\n",
      "Melbourne 6\n",
      "Melbourne 7\n",
      "Melbourne 8\n",
      "Melbourne 9\n",
      "Melbourne 10\n",
      "Melbourne 11\n",
      "Melbourne 12\n",
      "Melbourne 13\n",
      "Melbourne 14\n",
      "Melbourne 15\n",
      "Port Adelaide 1\n",
      "Port Adelaide 2\n",
      "Port Adelaide 3\n",
      "Port Adelaide 4\n",
      "Port Adelaide 5\n",
      "Port Adelaide 6\n",
      "Port Adelaide 7\n",
      "Port Adelaide 8\n",
      "Port Adelaide 9\n",
      "Port Adelaide 10\n",
      "Port Adelaide 11\n",
      "Port Adelaide 12\n",
      "Port Adelaide 13\n",
      "Port Adelaide 14\n",
      "Port Adelaide 15\n",
      "Collingwood 1\n",
      "Collingwood 2\n",
      "Collingwood 3\n",
      "Collingwood 4\n",
      "Collingwood 5\n",
      "Collingwood 6\n",
      "Collingwood 7\n",
      "Collingwood 8\n",
      "Collingwood 9\n",
      "Collingwood 10\n",
      "Collingwood 11\n",
      "Collingwood 12\n",
      "Collingwood 13\n",
      "Collingwood 14\n",
      "Collingwood 15\n",
      "Hawthorn 1\n",
      "Hawthorn 2\n",
      "Hawthorn 3\n",
      "Hawthorn 4\n",
      "Hawthorn 5\n",
      "Hawthorn 6\n",
      "Hawthorn 7\n",
      "Hawthorn 8\n",
      "Hawthorn 9\n",
      "Hawthorn 10\n",
      "Hawthorn 11\n",
      "Hawthorn 12\n",
      "Hawthorn 13\n",
      "Hawthorn 14\n",
      "Hawthorn 15\n",
      "Essendon 1\n",
      "Essendon 2\n",
      "Essendon 3\n",
      "Essendon 4\n",
      "Essendon 5\n",
      "Essendon 6\n",
      "Essendon 7\n",
      "Essendon 8\n",
      "Essendon 9\n",
      "Essendon 10\n",
      "Essendon 11\n",
      "Essendon 12\n",
      "Essendon 13\n",
      "Essendon 14\n",
      "Essendon 15\n",
      "Western Bulldogs 1\n",
      "Western Bulldogs 2\n",
      "Western Bulldogs 3\n",
      "Western Bulldogs 4\n",
      "Western Bulldogs 5\n",
      "Western Bulldogs 6\n",
      "Western Bulldogs 7\n",
      "Western Bulldogs 8\n",
      "Western Bulldogs 9\n",
      "Western Bulldogs 10\n",
      "Western Bulldogs 11\n",
      "Western Bulldogs 12\n",
      "Western Bulldogs 13\n",
      "Western Bulldogs 14\n",
      "Western Bulldogs 15\n",
      "St Kilda 1\n",
      "St Kilda 2\n",
      "St Kilda 3\n",
      "St Kilda 4\n",
      "St Kilda 5\n",
      "St Kilda 6\n",
      "St Kilda 7\n",
      "St Kilda 8\n",
      "St Kilda 9\n",
      "St Kilda 10\n",
      "St Kilda 11\n",
      "St Kilda 12\n",
      "St Kilda 13\n",
      "St Kilda 14\n",
      "St Kilda 15\n",
      "North Melbourne 1\n",
      "North Melbourne 2\n",
      "North Melbourne 3\n",
      "North Melbourne 4\n",
      "North Melbourne 5\n",
      "North Melbourne 6\n",
      "North Melbourne 7\n",
      "North Melbourne 8\n",
      "North Melbourne 9\n",
      "North Melbourne 10\n",
      "North Melbourne 11\n",
      "North Melbourne 12\n",
      "North Melbourne 13\n",
      "North Melbourne 14\n",
      "North Melbourne 15\n",
      "Fremantle 1\n",
      "Fremantle 2\n",
      "Fremantle 3\n",
      "Fremantle 4\n",
      "Fremantle 5\n",
      "Fremantle 6\n",
      "Fremantle 7\n",
      "Fremantle 8\n",
      "Fremantle 9\n",
      "Fremantle 10\n",
      "Fremantle 11\n",
      "Fremantle 12\n",
      "Fremantle 13\n",
      "Fremantle 14\n",
      "Fremantle 15\n",
      "Brisbane Lions 1\n",
      "Brisbane Lions 2\n",
      "Brisbane Lions 3\n",
      "Brisbane Lions 4\n",
      "Brisbane Lions 5\n",
      "Brisbane Lions 6\n",
      "Brisbane Lions 7\n",
      "Brisbane Lions 8\n",
      "Brisbane Lions 9\n",
      "Brisbane Lions 10\n",
      "Brisbane Lions 11\n",
      "Brisbane Lions 12\n",
      "Brisbane Lions 13\n",
      "Brisbane Lions 14\n",
      "Brisbane Lions 15\n",
      "Gold Coast 1\n",
      "Gold Coast 2\n",
      "Gold Coast 3\n",
      "Gold Coast 4\n",
      "Gold Coast 5\n",
      "Gold Coast 6\n",
      "Gold Coast 7\n",
      "Gold Coast 8\n",
      "Gold Coast 9\n",
      "Gold Coast 10\n",
      "Gold Coast 11\n",
      "Gold Coast 12\n",
      "Gold Coast 13\n",
      "Gold Coast 14\n",
      "Gold Coast 15\n",
      "Carlton 1\n",
      "Carlton 2\n",
      "Carlton 3\n",
      "Carlton 4\n",
      "Carlton 5\n",
      "Carlton 6\n",
      "Carlton 7\n",
      "Carlton 8\n",
      "Carlton 9\n",
      "Carlton 10\n",
      "Carlton 11\n",
      "Carlton 12\n",
      "Carlton 13\n",
      "Carlton 14\n",
      "Carlton 15\n"
     ]
    }
   ],
   "source": [
    "from data_prep.team_history import History\n",
    "team_df = History(mapping, proxyDict).generate_team_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 Richmond\n",
      "2018 West Coast\n",
      "2018 Sydney\n",
      "2018 Adelaide\n",
      "2018 Geelong\n",
      "2018 Greater Western Sydney\n",
      "2018 Melbourne\n",
      "2018 Port Adelaide\n",
      "2018 Collingwood\n",
      "2018 Hawthorn\n",
      "2018 Essendon\n",
      "2018 Western Bulldogs\n",
      "2018 St Kilda\n",
      "2018 North Melbourne\n",
      "2018 Fremantle\n",
      "2018 Brisbane Lions\n",
      "2018 Gold Coast\n",
      "2018 Carlton\n",
      "2017 Richmond\n",
      "2017 West Coast\n",
      "2017 Sydney\n",
      "2017 Adelaide\n",
      "2017 Geelong\n",
      "2017 Greater Western Sydney\n",
      "2017 Melbourne\n",
      "2017 Port Adelaide\n",
      "2017 Collingwood\n",
      "2017 Hawthorn\n",
      "2017 Essendon\n",
      "2017 Western Bulldogs\n",
      "2017 St Kilda\n",
      "2017 North Melbourne\n",
      "2017 Fremantle\n",
      "2017 Brisbane Lions\n",
      "2017 Gold Coast\n",
      "2017 Carlton\n",
      "2016 Richmond\n",
      "2016 West Coast\n",
      "2016 Sydney\n",
      "2016 Adelaide\n",
      "2016 Geelong\n",
      "2016 Greater Western Sydney\n",
      "2016 Melbourne\n",
      "2016 Port Adelaide\n",
      "2016 Collingwood\n",
      "2016 Hawthorn\n",
      "2016 Essendon\n",
      "2016 Western Bulldogs\n",
      "2016 St Kilda\n",
      "2016 North Melbourne\n",
      "2016 Fremantle\n",
      "2016 Brisbane Lions\n",
      "2016 Gold Coast\n",
      "2016 Carlton\n",
      "2015 Richmond\n",
      "2015 West Coast\n",
      "2015 Sydney\n",
      "2015 Adelaide\n",
      "2015 Geelong\n",
      "2015 Greater Western Sydney\n",
      "2015 Melbourne\n",
      "2015 Port Adelaide\n",
      "2015 Collingwood\n",
      "2015 Hawthorn\n",
      "2015 Essendon\n",
      "2015 Western Bulldogs\n",
      "2015 St Kilda\n",
      "2015 North Melbourne\n",
      "2015 Fremantle\n",
      "2015 Brisbane Lions\n",
      "2015 Gold Coast\n",
      "2015 Carlton\n",
      "2014 Richmond\n",
      "2014 West Coast\n",
      "2014 Sydney\n",
      "2014 Adelaide\n",
      "2014 Geelong\n",
      "2014 Greater Western Sydney\n",
      "2014 Melbourne\n",
      "2014 Port Adelaide\n",
      "2014 Collingwood\n",
      "2014 Hawthorn\n",
      "2014 Essendon\n",
      "2014 Western Bulldogs\n",
      "2014 St Kilda\n",
      "2014 North Melbourne\n",
      "2014 Fremantle\n",
      "2014 Brisbane Lions\n",
      "2014 Gold Coast\n",
      "2014 Carlton\n",
      "2013 Richmond\n",
      "2013 West Coast\n",
      "2013 Sydney\n",
      "2013 Adelaide\n",
      "2013 Geelong\n",
      "2013 Greater Western Sydney\n",
      "2013 Melbourne\n",
      "2013 Port Adelaide\n",
      "2013 Collingwood\n",
      "2013 Hawthorn\n",
      "2013 Essendon\n",
      "2013 Western Bulldogs\n",
      "2013 St Kilda\n",
      "2013 North Melbourne\n",
      "2013 Fremantle\n",
      "2013 Brisbane Lions\n",
      "2013 Gold Coast\n",
      "2013 Carlton\n",
      "2012 Richmond\n",
      "2012 West Coast\n",
      "2012 Sydney\n",
      "2012 Adelaide\n",
      "2012 Geelong\n",
      "2012 Greater Western Sydney\n",
      "2012 Melbourne\n",
      "2012 Port Adelaide\n",
      "2012 Collingwood\n",
      "2012 Hawthorn\n",
      "2012 Essendon\n",
      "2012 Western Bulldogs\n",
      "2012 St Kilda\n",
      "2012 North Melbourne\n",
      "2012 Fremantle\n",
      "2012 Brisbane Lions\n",
      "2012 Gold Coast\n",
      "2012 Carlton\n",
      "2011 Richmond\n",
      "2011 West Coast\n",
      "2011 Sydney\n",
      "2011 Adelaide\n",
      "2011 Geelong\n",
      "2011 Melbourne\n",
      "2011 Port Adelaide\n",
      "2011 Collingwood\n",
      "2011 Hawthorn\n",
      "2011 Essendon\n",
      "2011 Western Bulldogs\n",
      "2011 St Kilda\n",
      "2011 North Melbourne\n",
      "2011 Fremantle\n",
      "2011 Brisbane Lions\n",
      "2011 Gold Coast\n",
      "2011 Carlton\n",
      "2010 Richmond\n",
      "2010 West Coast\n",
      "2010 Sydney\n",
      "2010 Adelaide\n",
      "2010 Geelong\n",
      "2010 Melbourne\n",
      "2010 Port Adelaide\n",
      "2010 Collingwood\n",
      "2010 Hawthorn\n",
      "2010 Essendon\n",
      "2010 Western Bulldogs\n",
      "2010 St Kilda\n",
      "2010 North Melbourne\n",
      "2010 Fremantle\n",
      "2010 Brisbane Lions\n",
      "2010 Carlton\n",
      "2009 Richmond\n",
      "2009 West Coast\n",
      "2009 Sydney\n",
      "2009 Adelaide\n",
      "2009 Geelong\n",
      "2009 Melbourne\n",
      "2009 Port Adelaide\n",
      "2009 Collingwood\n",
      "2009 Hawthorn\n",
      "2009 Essendon\n",
      "2009 Western Bulldogs\n",
      "2009 St Kilda\n",
      "2009 North Melbourne\n",
      "2009 Fremantle\n",
      "2009 Brisbane Lions\n",
      "2009 Carlton\n",
      "2008 Richmond\n",
      "2008 West Coast\n",
      "2008 Sydney\n",
      "2008 Adelaide\n",
      "2008 Geelong\n",
      "2008 Melbourne\n",
      "2008 Port Adelaide\n",
      "2008 Collingwood\n",
      "2008 Hawthorn\n",
      "2008 Essendon\n",
      "2008 Western Bulldogs\n",
      "2008 St Kilda\n",
      "2008 North Melbourne\n",
      "2008 Fremantle\n",
      "2008 Brisbane Lions\n",
      "2008 Carlton\n",
      "2007 Richmond\n",
      "2007 West Coast\n",
      "2007 Sydney\n",
      "2007 Adelaide\n",
      "2007 Geelong\n",
      "2007 Melbourne\n",
      "2007 Port Adelaide\n",
      "2007 Collingwood\n",
      "2007 Hawthorn\n",
      "2007 Essendon\n",
      "2007 Western Bulldogs\n",
      "2007 St Kilda\n",
      "2007 North Melbourne\n",
      "2007 Fremantle\n",
      "2007 Brisbane Lions\n",
      "2007 Carlton\n",
      "2006 Richmond\n",
      "2006 West Coast\n",
      "2006 Sydney\n",
      "2006 Adelaide\n",
      "2006 Geelong\n",
      "2006 Melbourne\n",
      "2006 Port Adelaide\n",
      "2006 Collingwood\n",
      "2006 Hawthorn\n",
      "2006 Essendon\n",
      "2006 Western Bulldogs\n",
      "2006 St Kilda\n",
      "2006 North Melbourne\n",
      "2006 Fremantle\n",
      "2006 Brisbane Lions\n",
      "2006 Carlton\n",
      "2005 Richmond\n",
      "2005 West Coast\n",
      "2005 Sydney\n",
      "2005 Adelaide\n",
      "2005 Geelong\n",
      "2005 Melbourne\n",
      "2005 Port Adelaide\n",
      "2005 Collingwood\n",
      "2005 Hawthorn\n",
      "2005 Essendon\n",
      "2005 Western Bulldogs\n",
      "2005 St Kilda\n",
      "2005 North Melbourne\n",
      "2005 Fremantle\n",
      "2005 Brisbane Lions\n",
      "2005 Carlton\n",
      "2004 Richmond\n",
      "2004 West Coast\n",
      "2004 Sydney\n",
      "2004 Adelaide\n",
      "2004 Geelong\n",
      "2004 Melbourne\n",
      "2004 Port Adelaide\n",
      "2004 Collingwood\n",
      "2004 Hawthorn\n",
      "2004 Essendon\n",
      "2004 Western Bulldogs\n",
      "2004 St Kilda\n",
      "2004 North Melbourne\n",
      "2004 Fremantle\n",
      "2004 Brisbane Lions\n",
      "2004 Carlton\n",
      "2018 Richmond\n",
      "2018 West Coast\n",
      "2018 Sydney\n",
      "2018 Adelaide\n",
      "2018 Geelong\n",
      "2018 Greater Western Sydney\n",
      "2018 Melbourne\n",
      "2018 Port Adelaide\n",
      "2018 Collingwood\n",
      "2018 Hawthorn\n",
      "2018 Essendon\n",
      "2018 Western Bulldogs\n",
      "2018 St Kilda\n",
      "2018 North Melbourne\n",
      "2018 Fremantle\n",
      "2018 Brisbane Lions\n",
      "2018 Gold Coast\n",
      "2018 Carlton\n",
      "2017 Richmond\n",
      "2017 West Coast\n",
      "2017 Sydney\n",
      "2017 Adelaide\n",
      "2017 Geelong\n",
      "2017 Greater Western Sydney\n",
      "2017 Melbourne\n",
      "2017 Port Adelaide\n",
      "2017 Collingwood\n",
      "2017 Hawthorn\n",
      "2017 Essendon\n",
      "2017 Western Bulldogs\n",
      "2017 St Kilda\n",
      "2017 North Melbourne\n",
      "2017 Fremantle\n",
      "2017 Brisbane Lions\n",
      "2017 Gold Coast\n",
      "2017 Carlton\n",
      "2016 Richmond\n",
      "2016 West Coast\n",
      "2016 Sydney\n",
      "2016 Adelaide\n",
      "2016 Geelong\n",
      "2016 Greater Western Sydney\n",
      "2016 Melbourne\n",
      "2016 Port Adelaide\n",
      "2016 Collingwood\n",
      "2016 Hawthorn\n",
      "2016 Essendon\n",
      "2016 Western Bulldogs\n",
      "2016 St Kilda\n",
      "2016 North Melbourne\n",
      "2016 Fremantle\n",
      "2016 Brisbane Lions\n",
      "2016 Gold Coast\n",
      "2016 Carlton\n",
      "2015 Richmond\n",
      "2015 West Coast\n",
      "2015 Sydney\n",
      "2015 Adelaide\n",
      "2015 Geelong\n",
      "2015 Greater Western Sydney\n",
      "2015 Melbourne\n",
      "2015 Port Adelaide\n",
      "2015 Collingwood\n",
      "2015 Hawthorn\n",
      "2015 Essendon\n",
      "2015 Western Bulldogs\n",
      "2015 St Kilda\n",
      "2015 North Melbourne\n",
      "2015 Fremantle\n",
      "2015 Brisbane Lions\n",
      "2015 Gold Coast\n",
      "2015 Carlton\n",
      "2014 Richmond\n",
      "2014 West Coast\n",
      "2014 Sydney\n",
      "2014 Adelaide\n",
      "2014 Geelong\n",
      "2014 Greater Western Sydney\n",
      "2014 Melbourne\n",
      "2014 Port Adelaide\n",
      "2014 Collingwood\n",
      "2014 Hawthorn\n",
      "2014 Essendon\n",
      "2014 Western Bulldogs\n",
      "2014 St Kilda\n",
      "2014 North Melbourne\n",
      "2014 Fremantle\n",
      "2014 Brisbane Lions\n",
      "2014 Gold Coast\n",
      "2014 Carlton\n",
      "2013 Richmond\n",
      "2013 West Coast\n",
      "2013 Sydney\n",
      "2013 Adelaide\n",
      "2013 Geelong\n",
      "2013 Greater Western Sydney\n",
      "2013 Melbourne\n",
      "2013 Port Adelaide\n",
      "2013 Collingwood\n",
      "2013 Hawthorn\n",
      "2013 Essendon\n",
      "2013 Western Bulldogs\n",
      "2013 St Kilda\n",
      "2013 North Melbourne\n",
      "2013 Fremantle\n",
      "2013 Brisbane Lions\n",
      "2013 Gold Coast\n",
      "2013 Carlton\n",
      "2012 Richmond\n",
      "2012 West Coast\n",
      "2012 Sydney\n",
      "2012 Adelaide\n",
      "2012 Geelong\n",
      "2012 Greater Western Sydney\n",
      "2012 Melbourne\n",
      "2012 Port Adelaide\n",
      "2012 Collingwood\n",
      "2012 Hawthorn\n",
      "2012 Essendon\n",
      "2012 Western Bulldogs\n",
      "2012 St Kilda\n",
      "2012 North Melbourne\n",
      "2012 Fremantle\n",
      "2012 Brisbane Lions\n",
      "2012 Gold Coast\n",
      "2012 Carlton\n",
      "2011 Richmond\n",
      "2011 West Coast\n",
      "2011 Sydney\n",
      "2011 Adelaide\n",
      "2011 Geelong\n",
      "2011 Melbourne\n",
      "2011 Port Adelaide\n",
      "2011 Collingwood\n",
      "2011 Hawthorn\n",
      "2011 Essendon\n",
      "2011 Western Bulldogs\n",
      "2011 St Kilda\n",
      "2011 North Melbourne\n",
      "2011 Fremantle\n",
      "2011 Brisbane Lions\n",
      "2011 Gold Coast\n",
      "2011 Carlton\n",
      "2010 Richmond\n",
      "2010 West Coast\n",
      "2010 Sydney\n",
      "2010 Adelaide\n",
      "2010 Geelong\n",
      "2010 Melbourne\n",
      "2010 Port Adelaide\n",
      "2010 Collingwood\n",
      "2010 Hawthorn\n",
      "2010 Essendon\n",
      "2010 Western Bulldogs\n",
      "2010 St Kilda\n",
      "2010 North Melbourne\n",
      "2010 Fremantle\n",
      "2010 Brisbane Lions\n",
      "2010 Carlton\n",
      "2009 Richmond\n",
      "2009 West Coast\n",
      "2009 Sydney\n",
      "2009 Adelaide\n",
      "2009 Geelong\n",
      "2009 Melbourne\n",
      "2009 Port Adelaide\n",
      "2009 Collingwood\n",
      "2009 Hawthorn\n",
      "2009 Essendon\n",
      "2009 Western Bulldogs\n",
      "2009 St Kilda\n",
      "2009 North Melbourne\n",
      "2009 Fremantle\n",
      "2009 Brisbane Lions\n",
      "2009 Carlton\n",
      "2008 Richmond\n",
      "2008 West Coast\n",
      "2008 Sydney\n",
      "2008 Adelaide\n",
      "2008 Geelong\n",
      "2008 Melbourne\n",
      "2008 Port Adelaide\n",
      "2008 Collingwood\n",
      "2008 Hawthorn\n",
      "2008 Essendon\n",
      "2008 Western Bulldogs\n",
      "2008 St Kilda\n",
      "2008 North Melbourne\n",
      "2008 Fremantle\n",
      "2008 Brisbane Lions\n",
      "2008 Carlton\n",
      "2007 Richmond\n",
      "2007 West Coast\n",
      "2007 Sydney\n",
      "2007 Adelaide\n",
      "2007 Geelong\n",
      "2007 Melbourne\n",
      "2007 Port Adelaide\n",
      "2007 Collingwood\n",
      "2007 Hawthorn\n",
      "2007 Essendon\n",
      "2007 Western Bulldogs\n",
      "2007 St Kilda\n",
      "2007 North Melbourne\n",
      "2007 Fremantle\n",
      "2007 Brisbane Lions\n",
      "2007 Carlton\n",
      "2006 Richmond\n",
      "2006 West Coast\n",
      "2006 Sydney\n",
      "2006 Adelaide\n",
      "2006 Geelong\n",
      "2006 Melbourne\n",
      "2006 Port Adelaide\n",
      "2006 Collingwood\n",
      "2006 Hawthorn\n",
      "2006 Essendon\n",
      "2006 Western Bulldogs\n",
      "2006 St Kilda\n",
      "2006 North Melbourne\n",
      "2006 Fremantle\n",
      "2006 Brisbane Lions\n",
      "2006 Carlton\n",
      "2005 Richmond\n",
      "2005 West Coast\n",
      "2005 Sydney\n",
      "2005 Adelaide\n",
      "2005 Geelong\n",
      "2005 Melbourne\n",
      "2005 Port Adelaide\n",
      "2005 Collingwood\n",
      "2005 Hawthorn\n",
      "2005 Essendon\n",
      "2005 Western Bulldogs\n",
      "2005 St Kilda\n",
      "2005 North Melbourne\n",
      "2005 Fremantle\n",
      "2005 Brisbane Lions\n",
      "2005 Carlton\n",
      "2004 Richmond\n",
      "2004 West Coast\n",
      "2004 Sydney\n",
      "2004 Adelaide\n",
      "2004 Geelong\n",
      "2004 Melbourne\n",
      "2004 Port Adelaide\n",
      "2004 Collingwood\n",
      "2004 Hawthorn\n",
      "2004 Essendon\n",
      "2004 Western Bulldogs\n",
      "2004 St Kilda\n",
      "2004 North Melbourne\n",
      "2004 Fremantle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2004 Brisbane Lions\n",
      "2004 Carlton\n"
     ]
    }
   ],
   "source": [
    "from data_prep.team_history import History\n",
    "History(mapping, proxyDict).generate_game_data('training-all/', team_df)\n",
    "History(mapping, proxyDict).generate_game_data_ha('training-hva/', team_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2789, 33)\n",
      "(2789,)\n",
      "Wins vs losses 0.577626389386877\n"
     ]
    }
   ],
   "source": [
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for i in range(1,16):\n",
    "    X = np.concatenate([\n",
    "        np.load('training-all/training-'+str(2019-i)+'.npy'),\n",
    "        np.load('training-hva/training-'+str(2019-i)+'.npy')[:,1:] #remove the rnd column\n",
    "                ], axis=1)\n",
    "    mask = np.isnan(X).any(axis=1)\n",
    "    index = np.where(mask==True)[0][0] ## X8 has a row containing nulls\n",
    "    X = np.delete(X, index, 0)\n",
    "    X_list.append(X)\n",
    "\n",
    "    y = np.load('training-all/results-'+str(2019-i)+'.npy')\n",
    "    y = np.delete(y, index, 0)\n",
    "    y_list.append(y)\n",
    "    \n",
    "X = np.concatenate(X_list, axis=0)\n",
    "y = np.concatenate(y_list, axis=0)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print('Wins vs losses',np.sum(y)/float(len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rnd</th>\n",
       "      <th>h_F_mean</th>\n",
       "      <th>h_F_std</th>\n",
       "      <th>h_A_mean</th>\n",
       "      <th>h_A_std</th>\n",
       "      <th>h_M_mean</th>\n",
       "      <th>h_M_std</th>\n",
       "      <th>h_R_mean</th>\n",
       "      <th>h_perc</th>\n",
       "      <th>a_F_mean</th>\n",
       "      <th>a_F_std</th>\n",
       "      <th>a_A_mean</th>\n",
       "      <th>a_A_std</th>\n",
       "      <th>a_M_mean</th>\n",
       "      <th>a_M_std</th>\n",
       "      <th>a_R_mean</th>\n",
       "      <th>a_perc</th>\n",
       "      <th>h_F_mean_hva</th>\n",
       "      <th>h_F_std_hva</th>\n",
       "      <th>h_A_mean_hva</th>\n",
       "      <th>h_A_std_hva</th>\n",
       "      <th>h_M_mean_hva</th>\n",
       "      <th>h_M_std_hva</th>\n",
       "      <th>h_R_mean_hva</th>\n",
       "      <th>h_perc_hva</th>\n",
       "      <th>a_F_mean_hva</th>\n",
       "      <th>a_F_std_hva</th>\n",
       "      <th>a_A_mean_hva</th>\n",
       "      <th>a_A_std_hva</th>\n",
       "      <th>a_M_mean_hva</th>\n",
       "      <th>a_M_std_hva</th>\n",
       "      <th>a_R_mean_hva</th>\n",
       "      <th>a_perc_hva</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>101.500000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>106.500000</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.953052</td>\n",
       "      <td>109.500000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.190217</td>\n",
       "      <td>121.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.273684</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.008547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>101.666667</td>\n",
       "      <td>15.923428</td>\n",
       "      <td>100.666667</td>\n",
       "      <td>12.498889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.498889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.009934</td>\n",
       "      <td>82.666667</td>\n",
       "      <td>7.363574</td>\n",
       "      <td>101.333333</td>\n",
       "      <td>4.189935</td>\n",
       "      <td>-18.666667</td>\n",
       "      <td>4.189935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>111.50</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>92.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>19.50</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.211957</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.852941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>12.192894</td>\n",
       "      <td>74.166667</td>\n",
       "      <td>32.121730</td>\n",
       "      <td>30.833333</td>\n",
       "      <td>32.121730</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.415730</td>\n",
       "      <td>83.666667</td>\n",
       "      <td>21.898757</td>\n",
       "      <td>82.166667</td>\n",
       "      <td>17.686310</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>17.686310</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.018256</td>\n",
       "      <td>111.00</td>\n",
       "      <td>7.788881</td>\n",
       "      <td>67.0</td>\n",
       "      <td>35.440090</td>\n",
       "      <td>44.00</td>\n",
       "      <td>35.440090</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.656716</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>19.442222</td>\n",
       "      <td>86.666667</td>\n",
       "      <td>17.461068</td>\n",
       "      <td>-17.666667</td>\n",
       "      <td>17.461068</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.796154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>100.555556</td>\n",
       "      <td>13.953185</td>\n",
       "      <td>75.555556</td>\n",
       "      <td>34.944754</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>34.944754</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.330882</td>\n",
       "      <td>65.111111</td>\n",
       "      <td>17.329772</td>\n",
       "      <td>93.444444</td>\n",
       "      <td>10.510430</td>\n",
       "      <td>-28.333333</td>\n",
       "      <td>10.510430</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.696790</td>\n",
       "      <td>110.75</td>\n",
       "      <td>6.759253</td>\n",
       "      <td>58.5</td>\n",
       "      <td>34.040417</td>\n",
       "      <td>52.25</td>\n",
       "      <td>34.040417</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.893162</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>6.041523</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>5.744563</td>\n",
       "      <td>-41.000000</td>\n",
       "      <td>5.744563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.563830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.0</td>\n",
       "      <td>97.307692</td>\n",
       "      <td>17.397545</td>\n",
       "      <td>72.076923</td>\n",
       "      <td>30.406127</td>\n",
       "      <td>25.230769</td>\n",
       "      <td>30.406127</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>1.350053</td>\n",
       "      <td>88.307692</td>\n",
       "      <td>18.938705</td>\n",
       "      <td>70.230769</td>\n",
       "      <td>14.337438</td>\n",
       "      <td>18.076923</td>\n",
       "      <td>14.337438</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>1.257393</td>\n",
       "      <td>109.60</td>\n",
       "      <td>6.468385</td>\n",
       "      <td>62.2</td>\n",
       "      <td>31.333050</td>\n",
       "      <td>47.40</td>\n",
       "      <td>31.333050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.762058</td>\n",
       "      <td>93.166667</td>\n",
       "      <td>20.586538</td>\n",
       "      <td>68.166667</td>\n",
       "      <td>12.811670</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>12.811670</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.366748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rnd    h_F_mean    h_F_std    h_A_mean    h_A_std   h_M_mean    h_M_std  \\\n",
       "0   3.0  101.500000  19.500000  106.500000  11.500000  -5.000000  11.500000   \n",
       "1   4.0  101.666667  15.923428  100.666667  12.498889   1.000000  12.498889   \n",
       "2   7.0  105.000000  12.192894   74.166667  32.121730  30.833333  32.121730   \n",
       "3  10.0  100.555556  13.953185   75.555556  34.944754  25.000000  34.944754   \n",
       "4  15.0   97.307692  17.397545   72.076923  30.406127  25.230769  30.406127   \n",
       "\n",
       "   h_R_mean    h_perc    a_F_mean    a_F_std    a_A_mean    a_A_std  \\\n",
       "0  0.500000  0.953052  109.500000   8.500000   92.000000  25.000000   \n",
       "1  0.666667  1.009934   82.666667   7.363574  101.333333   4.189935   \n",
       "2  0.833333  1.415730   83.666667  21.898757   82.166667  17.686310   \n",
       "3  0.777778  1.330882   65.111111  17.329772   93.444444  10.510430   \n",
       "4  0.769231  1.350053   88.307692  18.938705   70.230769  14.337438   \n",
       "\n",
       "    a_M_mean    a_M_std  a_R_mean    a_perc  h_F_mean_hva  h_F_std_hva  \\\n",
       "0  17.500000  25.000000  1.000000  1.190217        121.00     0.000000   \n",
       "1 -18.666667   4.189935  0.000000  0.815789        111.50     9.500000   \n",
       "2   1.500000  17.686310  0.500000  1.018256        111.00     7.788881   \n",
       "3 -28.333333  10.510430  0.222222  0.696790        110.75     6.759253   \n",
       "4  18.076923  14.337438  0.769231  1.257393        109.60     6.468385   \n",
       "\n",
       "   h_A_mean_hva  h_A_std_hva  h_M_mean_hva  h_M_std_hva  h_R_mean_hva  \\\n",
       "0          95.0     0.000000         26.00     0.000000           1.0   \n",
       "1          92.0     3.000000         19.50     3.000000           1.0   \n",
       "2          67.0    35.440090         44.00    35.440090           1.0   \n",
       "3          58.5    34.040417         52.25    34.040417           1.0   \n",
       "4          62.2    31.333050         47.40    31.333050           1.0   \n",
       "\n",
       "   h_perc_hva  a_F_mean_hva  a_F_std_hva  a_A_mean_hva  a_A_std_hva  \\\n",
       "0    1.273684    118.000000     0.000000    117.000000     0.000000   \n",
       "1    1.211957     87.000000     5.000000    102.000000     5.000000   \n",
       "2    1.656716     69.000000    19.442222     86.666667    17.461068   \n",
       "3    1.893162     53.000000     6.041523     94.000000     5.744563   \n",
       "4    1.762058     93.166667    20.586538     68.166667    12.811670   \n",
       "\n",
       "   a_M_mean_hva  a_M_std_hva  a_R_mean_hva  a_perc_hva  \n",
       "0      1.000000     0.000000      1.000000    1.008547  \n",
       "1    -15.000000     5.000000      0.000000    0.852941  \n",
       "2    -17.666667    17.461068      0.333333    0.796154  \n",
       "3    -41.000000     5.744563      0.000000    0.563830  \n",
       "4     25.000000    12.811670      1.000000    1.366748  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_prep.feature_eng import Features\n",
    "# training_cols = Features().training_cols()\n",
    "# pd.DataFrame(Features().div_cols(X_list[0]), columns=training_cols).head()\n",
    "cols = Features().cols()\n",
    "pd.DataFrame(X_list[0], columns=cols).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Season 2018\n",
      "Bayes CV search took 59.14 seconds for 25 candidates parameter settings.\n",
      "val. score: 63.602429429289515\n",
      "test score: 26.095565013073692\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=1.0, colsample_bynode=0.1, colsample_bytree=1.0,\n",
      "       gamma=0, learning_rate=0.005296220140853066, max_delta_step=0,\n",
      "       max_depth=4, min_child_weight=1, missing=None, n_estimators=500,\n",
      "       n_jobs=-1, nthread=None, objective='binary:logistic',\n",
      "       random_state=0, reg_alpha=1.0, reg_lambda=1.0, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=0.9522291522845555)\n",
      "\n",
      "Interrupting!\n",
      "Season 2017\n",
      "Bayes CV search took 30.47 seconds for 25 candidates parameter settings.\n",
      "val. score: 66.45966446589514\n",
      "test score: 11.403725030836588\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=0.39180518217263116,\n",
      "       colsample_bynode=0.5687587620579737,\n",
      "       colsample_bytree=0.7588024214699526, gamma=0,\n",
      "       learning_rate=0.005857004069980938, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=430, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.2829306051851955, reg_lambda=1.0, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=0.19346266174819096)\n",
      "\n",
      "Season 2016\n",
      "Bayes CV search took 60.28 seconds for 25 candidates parameter settings.\n",
      "val. score: 59.341247321526566\n",
      "test score: 39.52034681940387\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=0.9095417575026508,\n",
      "       colsample_bynode=0.4209095524195543,\n",
      "       colsample_bytree=0.30219949653065936, gamma=0,\n",
      "       learning_rate=0.017070578585355652, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=194, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.4601081088910607, reg_lambda=0.5021460025598551,\n",
      "       scale_pos_weight=1, seed=None, silent=True,\n",
      "       subsample=0.9865000616750151)\n",
      "\n",
      "Interrupting!\n",
      "Season 2015\n",
      "Bayes CV search took 62.33 seconds for 25 candidates parameter settings.\n",
      "val. score: 65.59481854842659\n",
      "test score: 17.30789411025552\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=0.7323994246531157,\n",
      "       colsample_bynode=0.9659738684697661,\n",
      "       colsample_bytree=0.7309540107969559, gamma=0,\n",
      "       learning_rate=0.009937203097315658, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=292, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.0, reg_lambda=0.4709883589147624, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=0.7743332711983313)\n",
      "\n",
      "Interrupting!\n",
      "Season 2014\n",
      "Bayes CV search took 44.59 seconds for 25 candidates parameter settings.\n",
      "val. score: 65.0003596581704\n",
      "test score: 19.668132487166815\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=0.4692091416038848,\n",
      "       colsample_bynode=0.9290587534171381,\n",
      "       colsample_bytree=0.9510486300413493, gamma=0,\n",
      "       learning_rate=0.007885729454305731, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=311, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.7344502448768271, reg_lambda=0.5106959797194698,\n",
      "       scale_pos_weight=1, seed=None, silent=True,\n",
      "       subsample=0.2834194312751356)\n",
      "\n",
      "Season 2013\n",
      "Bayes CV search took 57.44 seconds for 25 candidates parameter settings.\n",
      "val. score: 64.21944674665242\n",
      "test score: 31.61584014597023\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=0.575479585946528,\n",
      "       colsample_bynode=0.29548284963522065,\n",
      "       colsample_bytree=0.93833507615037, gamma=0,\n",
      "       learning_rate=0.007194348983328539, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=415, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.42422031894915513, reg_lambda=0.024072373283807298,\n",
      "       scale_pos_weight=1, seed=None, silent=True,\n",
      "       subsample=0.397875986448476)\n",
      "\n",
      "Season 2012\n",
      "Bayes CV search took 54.02 seconds for 25 candidates parameter settings.\n",
      "val. score: 61.41326843060039\n",
      "test score: 37.000962394418266\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=0.6140887519022181,\n",
      "       colsample_bynode=0.464213109402395,\n",
      "       colsample_bytree=0.6917075422214957, gamma=0,\n",
      "       learning_rate=0.010501718786907215, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=248, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=1.0, reg_lambda=0.41404751675981977, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=0.7305989371369932)\n",
      "\n",
      "Season 2011\n",
      "Bayes CV search took 60.05 seconds for 25 candidates parameter settings.\n",
      "val. score: 60.18765590520161\n",
      "test score: 38.37187540453277\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=0.6833235375437186,\n",
      "       colsample_bynode=0.7574781634096506,\n",
      "       colsample_bytree=0.7403717216791443, gamma=0,\n",
      "       learning_rate=0.013382903120112552, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=178, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.1733802484395738, reg_lambda=0.5403568316761923,\n",
      "       scale_pos_weight=1, seed=None, silent=True,\n",
      "       subsample=0.6198040318232086)\n",
      "\n",
      "Interrupting!\n",
      "Season 2010\n",
      "Bayes CV search took 20.31 seconds for 25 candidates parameter settings.\n",
      "val. score: 66.22404800915254\n",
      "test score: 15.78062438766195\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=0.3039348368701069,\n",
      "       colsample_bynode=0.49979736990288404, colsample_bytree=1.0, gamma=0,\n",
      "       learning_rate=0.007031115101013226, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=500, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.5367022527808462, reg_lambda=0.09495917027518726,\n",
      "       scale_pos_weight=1, seed=None, silent=True, subsample=0.1)\n",
      "\n",
      "Season 2009\n",
      "Bayes CV search took 56.96 seconds for 25 candidates parameter settings.\n",
      "val. score: 64.71829792878964\n",
      "test score: 22.244910404635952\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=0.32672490509782814,\n",
      "       colsample_bynode=0.42785655412508716,\n",
      "       colsample_bytree=0.8176956843682203, gamma=0,\n",
      "       learning_rate=0.007517550091491719, max_delta_step=0, max_depth=4,\n",
      "       min_child_weight=1, missing=None, n_estimators=362, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.0, reg_lambda=0.46682123576141427, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=0.906231902725296)\n",
      "\n",
      "Season 2008\n",
      "Bayes CV search took 50.74 seconds for 25 candidates parameter settings.\n",
      "val. score: 64.24630597644644\n",
      "test score: 23.445520664802608\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=0.7067397767147323,\n",
      "       colsample_bynode=0.3513910180707448,\n",
      "       colsample_bytree=0.8993378729748105, gamma=0,\n",
      "       learning_rate=0.012510898003966483, max_delta_step=0, max_depth=4,\n",
      "       min_child_weight=1, missing=None, n_estimators=176, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.3845876885713235, reg_lambda=0.16342908720556906,\n",
      "       scale_pos_weight=1, seed=None, silent=True,\n",
      "       subsample=0.5062132933471052)\n",
      "\n",
      "Interrupting!\n",
      "Season 2007\n",
      "Bayes CV search took 33.11 seconds for 25 candidates parameter settings.\n",
      "val. score: 65.82589719005014\n",
      "test score: 17.345101868514323\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=0.8578771585636734,\n",
      "       colsample_bynode=0.9997574124473029,\n",
      "       colsample_bytree=0.5315936174250775, gamma=0,\n",
      "       learning_rate=0.005143409375301899, max_delta_step=0, max_depth=4,\n",
      "       min_child_weight=1, missing=None, n_estimators=465, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.5570964338241898, reg_lambda=0.187851959918247,\n",
      "       scale_pos_weight=1, seed=None, silent=True,\n",
      "       subsample=0.5594780035505067)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Season 2006\n",
      "Bayes CV search took 45.12 seconds for 25 candidates parameter settings.\n",
      "val. score: 64.60607337939496\n",
      "test score: 10.62795083854288\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=0.1, colsample_bynode=0.6313560789329671,\n",
      "       colsample_bytree=0.18745323543193845, gamma=0,\n",
      "       learning_rate=0.010108844990707157, max_delta_step=0, max_depth=4,\n",
      "       min_child_weight=1, missing=None, n_estimators=500, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.23654305397611725, reg_lambda=0.8974360735916687,\n",
      "       scale_pos_weight=1, seed=None, silent=True,\n",
      "       subsample=0.5702113299859627)\n",
      "\n",
      "Interrupting!\n",
      "Season 2005\n",
      "Bayes CV search took 1.27 seconds for 25 candidates parameter settings.\n",
      "val. score: 68.34427456401949\n",
      "test score: 16.288474185398602\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=0.629978735449696,\n",
      "       colsample_bynode=0.8165265082599713,\n",
      "       colsample_bytree=0.8655532084813427, gamma=0,\n",
      "       learning_rate=0.02002052482115019, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=118, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.9543802674636961, reg_lambda=0.25848577529761513,\n",
      "       scale_pos_weight=1, seed=None, silent=True,\n",
      "       subsample=0.7250183566471105)\n",
      "\n",
      "Interrupting!\n",
      "Season 2004\n",
      "Bayes CV search took 21.00 seconds for 25 candidates parameter settings.\n",
      "val. score: 65.30677044051804\n",
      "test score: 20.442591587412522\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=0.33327884438070193,\n",
      "       colsample_bynode=0.6220541529156852,\n",
      "       colsample_bytree=0.6390483956378086, gamma=0,\n",
      "       learning_rate=0.010237632948678903, max_delta_step=0, max_depth=4,\n",
      "       min_child_weight=1, missing=None, n_estimators=370, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.1652165412662612, reg_lambda=0.5402140712395047,\n",
      "       scale_pos_weight=1, seed=None, silent=True,\n",
      "       subsample=0.5455823551496909)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from training.training import Training\n",
    "scores, best_models = Training(25, 65).train(X_list, y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.14396768950844\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>h_M_mean</th>\n",
       "      <td>0.055232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h_perc</th>\n",
       "      <td>0.054716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_M_mean</th>\n",
       "      <td>0.051450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_perc</th>\n",
       "      <td>0.049033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h_R_mean</th>\n",
       "      <td>0.048877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_A_mean</th>\n",
       "      <td>0.042898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_R_mean</th>\n",
       "      <td>0.039392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h_A_mean</th>\n",
       "      <td>0.038964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h_perc_hva</th>\n",
       "      <td>0.034529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h_M_mean_hva</th>\n",
       "      <td>0.033626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              importance\n",
       "h_M_mean        0.055232\n",
       "h_perc          0.054716\n",
       "a_M_mean        0.051450\n",
       "a_perc          0.049033\n",
       "h_R_mean        0.048877\n",
       "a_A_mean        0.042898\n",
       "a_R_mean        0.039392\n",
       "h_A_mean        0.038964\n",
       "h_perc_hva      0.034529\n",
       "h_M_mean_hva    0.033626"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def averagingImp(models=[]):\n",
    "    predictions = np.column_stack([\n",
    "        model.feature_importances_ for model in models\n",
    "    ])\n",
    "    return np.mean(predictions, axis=1)\n",
    "\n",
    "imp = pd.DataFrame(data=averagingImp(best_models),\n",
    "             index=cols, \n",
    "             columns=['importance']).sort_values(by=['importance'], ascending=False)\n",
    "imp.iloc[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averagingModels(X, models=[]):\n",
    "    predictions = np.column_stack([\n",
    "        model.predict_proba(X)[:,1] for model in models\n",
    "    ])\n",
    "    return np.mean(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1251c5f28>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12.5,4))\n",
    "line_height = 2\n",
    "data = averagingModels(np.concatenate(X_list, axis=0), best_models)\n",
    "x = plt.hist(data, bins='auto', density=True, facecolor='green', alpha=0.5)\n",
    "\n",
    "plt.vlines(np.percentile(data, 5), 0, line_height, linestyle=\"--\", colors='green', label=\"5%\")\n",
    "plt.vlines(np.percentile(data, 50), 0, line_height, linestyle=\"--\", colors='red', label=\"50%\")\n",
    "plt.vlines(np.percentile(data, 95), 0, line_height, linestyle=\"--\", colors='blue', label=\"95%\")\n",
    "plt.xlabel('Probability')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_score</th>\n",
       "      <th>booster</th>\n",
       "      <th>colsample_bylevel</th>\n",
       "      <th>colsample_bynode</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>gamma</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_delta_step</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>missing</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>n_jobs</th>\n",
       "      <th>nthread</th>\n",
       "      <th>objective</th>\n",
       "      <th>random_state</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>scale_pos_weight</th>\n",
       "      <th>seed</th>\n",
       "      <th>silent</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005296</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.952229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>0.391805</td>\n",
       "      <td>0.568759</td>\n",
       "      <td>0.758802</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005857</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>430</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.282931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.193463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>0.909542</td>\n",
       "      <td>0.420910</td>\n",
       "      <td>0.302199</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017071</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>194</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.460108</td>\n",
       "      <td>0.502146</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.986500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>0.732399</td>\n",
       "      <td>0.965974</td>\n",
       "      <td>0.730954</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009937</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>292</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.470988</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.774333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>0.469209</td>\n",
       "      <td>0.929059</td>\n",
       "      <td>0.951049</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007886</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>311</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.734450</td>\n",
       "      <td>0.510696</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.283419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>0.575480</td>\n",
       "      <td>0.295483</td>\n",
       "      <td>0.938335</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007194</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>415</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.424220</td>\n",
       "      <td>0.024072</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.397876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>0.614089</td>\n",
       "      <td>0.464213</td>\n",
       "      <td>0.691708</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010502</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>248</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.414048</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.730599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>0.683324</td>\n",
       "      <td>0.757478</td>\n",
       "      <td>0.740372</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013383</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>178</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.173380</td>\n",
       "      <td>0.540357</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.619804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>0.303935</td>\n",
       "      <td>0.499797</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.536702</td>\n",
       "      <td>0.094959</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>0.326725</td>\n",
       "      <td>0.427857</td>\n",
       "      <td>0.817696</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007518</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>362</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.466821</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.906232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>0.706740</td>\n",
       "      <td>0.351391</td>\n",
       "      <td>0.899338</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012511</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>176</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.384588</td>\n",
       "      <td>0.163429</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.506213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>0.857877</td>\n",
       "      <td>0.999757</td>\n",
       "      <td>0.531594</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005143</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>465</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.557096</td>\n",
       "      <td>0.187852</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.559478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.631356</td>\n",
       "      <td>0.187453</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010109</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236543</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.570211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>0.629979</td>\n",
       "      <td>0.816527</td>\n",
       "      <td>0.865553</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020021</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>118</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.954380</td>\n",
       "      <td>0.258486</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.725018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>0.333279</td>\n",
       "      <td>0.622054</td>\n",
       "      <td>0.639048</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010238</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>370</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.165217</td>\n",
       "      <td>0.540214</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.545582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    base_score booster  colsample_bylevel  colsample_bynode  colsample_bytree  \\\n",
       "0     0.575746  gbtree           1.000000          0.100000          1.000000   \n",
       "1     0.575746  gbtree           0.391805          0.568759          0.758802   \n",
       "2     0.575746  gbtree           0.909542          0.420910          0.302199   \n",
       "3     0.575746  gbtree           0.732399          0.965974          0.730954   \n",
       "4     0.575746  gbtree           0.469209          0.929059          0.951049   \n",
       "5     0.575746  gbtree           0.575480          0.295483          0.938335   \n",
       "6     0.575746  gbtree           0.614089          0.464213          0.691708   \n",
       "7     0.575746  gbtree           0.683324          0.757478          0.740372   \n",
       "8     0.575746  gbtree           0.303935          0.499797          1.000000   \n",
       "9     0.575746  gbtree           0.326725          0.427857          0.817696   \n",
       "10    0.575746  gbtree           0.706740          0.351391          0.899338   \n",
       "11    0.575746  gbtree           0.857877          0.999757          0.531594   \n",
       "12    0.575746  gbtree           0.100000          0.631356          0.187453   \n",
       "13    0.575746  gbtree           0.629979          0.816527          0.865553   \n",
       "14    0.575746  gbtree           0.333279          0.622054          0.639048   \n",
       "\n",
       "    gamma  learning_rate  max_delta_step  max_depth  min_child_weight missing  \\\n",
       "0       0       0.005296               0          4                 1    None   \n",
       "1       0       0.005857               0          5                 1    None   \n",
       "2       0       0.017071               0          5                 1    None   \n",
       "3       0       0.009937               0          5                 1    None   \n",
       "4       0       0.007886               0          5                 1    None   \n",
       "5       0       0.007194               0          3                 1    None   \n",
       "6       0       0.010502               0          3                 1    None   \n",
       "7       0       0.013383               0          3                 1    None   \n",
       "8       0       0.007031               0          3                 1    None   \n",
       "9       0       0.007518               0          4                 1    None   \n",
       "10      0       0.012511               0          4                 1    None   \n",
       "11      0       0.005143               0          4                 1    None   \n",
       "12      0       0.010109               0          4                 1    None   \n",
       "13      0       0.020021               0          3                 1    None   \n",
       "14      0       0.010238               0          4                 1    None   \n",
       "\n",
       "    n_estimators  n_jobs nthread        objective  random_state  reg_alpha  \\\n",
       "0            500      -1    None  binary:logistic             0   1.000000   \n",
       "1            430      -1    None  binary:logistic             0   0.282931   \n",
       "2            194      -1    None  binary:logistic             0   0.460108   \n",
       "3            292      -1    None  binary:logistic             0   0.000000   \n",
       "4            311      -1    None  binary:logistic             0   0.734450   \n",
       "5            415      -1    None  binary:logistic             0   0.424220   \n",
       "6            248      -1    None  binary:logistic             0   1.000000   \n",
       "7            178      -1    None  binary:logistic             0   0.173380   \n",
       "8            500      -1    None  binary:logistic             0   0.536702   \n",
       "9            362      -1    None  binary:logistic             0   0.000000   \n",
       "10           176      -1    None  binary:logistic             0   0.384588   \n",
       "11           465      -1    None  binary:logistic             0   0.557096   \n",
       "12           500      -1    None  binary:logistic             0   0.236543   \n",
       "13           118      -1    None  binary:logistic             0   0.954380   \n",
       "14           370      -1    None  binary:logistic             0   0.165217   \n",
       "\n",
       "    reg_lambda  scale_pos_weight  seed  silent  subsample  \n",
       "0     1.000000                 1  None    True   0.952229  \n",
       "1     1.000000                 1  None    True   0.193463  \n",
       "2     0.502146                 1  None    True   0.986500  \n",
       "3     0.470988                 1  None    True   0.774333  \n",
       "4     0.510696                 1  None    True   0.283419  \n",
       "5     0.024072                 1  None    True   0.397876  \n",
       "6     0.414048                 1  None    True   0.730599  \n",
       "7     0.540357                 1  None    True   0.619804  \n",
       "8     0.094959                 1  None    True   0.100000  \n",
       "9     0.466821                 1  None    True   0.906232  \n",
       "10    0.163429                 1  None    True   0.506213  \n",
       "11    0.187852                 1  None    True   0.559478  \n",
       "12    0.897436                 1  None    True   0.570211  \n",
       "13    0.258486                 1  None    True   0.725018  \n",
       "14    0.540214                 1  None    True   0.545582  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for model in best_models:\n",
    "    rows.append(model.get_params())\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/d719674/dev/code/AFL-Monash-comp/data_prep/web_scraping.py:35: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 35 of the file /Users/d719674/dev/code/AFL-Monash-comp/data_prep/web_scraping.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(r.text, \"html\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Sydney', 'Collingwood'],\n",
       " ['Hawthorn', 'Port Adelaide'],\n",
       " ['Western Bulldogs', 'North Melbourne'],\n",
       " ['Adelaide', 'West Coast'],\n",
       " ['Gold Coast', 'Geelong'],\n",
       " ['Richmond', 'Essendon'],\n",
       " ['Melbourne', 'Greater Western Sydney'],\n",
       " ['St Kilda', 'Carlton'],\n",
       " ['Fremantle', 'Brisbane Lions']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_prep.web_scraping import Scrape\n",
    "\n",
    "games = []\n",
    "start = 110\n",
    "for i in range(start,start+9):\n",
    "    games.append(Scrape(mapping, proxyDict).scrape_game(i))\n",
    "games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_prep.scoring import Scoring\n",
    "scoring = Scoring(mapping, proxyDict).score_data(games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h_M_mean</th>\n",
       "      <th>h_perc</th>\n",
       "      <th>a_M_mean</th>\n",
       "      <th>a_perc</th>\n",
       "      <th>h_R_mean</th>\n",
       "      <th>a_A_mean</th>\n",
       "      <th>a_R_mean</th>\n",
       "      <th>h_A_mean</th>\n",
       "      <th>h_perc_hva</th>\n",
       "      <th>h_M_mean_hva</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-13.444444</td>\n",
       "      <td>0.848750</td>\n",
       "      <td>21.555556</td>\n",
       "      <td>1.295282</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>0.778947</td>\n",
       "      <td>-21.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.222222</td>\n",
       "      <td>0.997218</td>\n",
       "      <td>6.111111</td>\n",
       "      <td>1.077356</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>79.888889</td>\n",
       "      <td>1.028846</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.964143</td>\n",
       "      <td>-16.777778</td>\n",
       "      <td>0.820452</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>93.444444</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>83.666667</td>\n",
       "      <td>1.084469</td>\n",
       "      <td>6.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.666667</td>\n",
       "      <td>1.152866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>75.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>69.777778</td>\n",
       "      <td>1.127820</td>\n",
       "      <td>8.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-19.444444</td>\n",
       "      <td>0.759615</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1.461538</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>69.333333</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>80.888889</td>\n",
       "      <td>0.843206</td>\n",
       "      <td>-11.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.888889</td>\n",
       "      <td>1.034946</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.008427</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>79.111111</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>82.666667</td>\n",
       "      <td>1.205036</td>\n",
       "      <td>14.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-21.666667</td>\n",
       "      <td>0.762195</td>\n",
       "      <td>21.555556</td>\n",
       "      <td>1.291291</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>91.111111</td>\n",
       "      <td>0.795337</td>\n",
       "      <td>-19.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-8.888889</td>\n",
       "      <td>0.894040</td>\n",
       "      <td>-22.333333</td>\n",
       "      <td>0.763807</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>94.555556</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>83.888889</td>\n",
       "      <td>0.878698</td>\n",
       "      <td>-10.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.222222</td>\n",
       "      <td>1.103503</td>\n",
       "      <td>3.111111</td>\n",
       "      <td>1.035132</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>88.555556</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>69.777778</td>\n",
       "      <td>1.265574</td>\n",
       "      <td>20.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    h_M_mean    h_perc   a_M_mean    a_perc  h_R_mean   a_A_mean  a_R_mean  \\\n",
       "0 -13.444444  0.848750  21.555556  1.295282  0.333333  73.000000  0.777778   \n",
       "1  -0.222222  0.997218   6.111111  1.077356  0.444444  79.000000  0.555556   \n",
       "2  -3.000000  0.964143 -16.777778  0.820452  0.444444  93.444444  0.222222   \n",
       "3  10.666667  1.152866   0.000000  1.000000  0.555556  75.666667  0.666667   \n",
       "4 -19.444444  0.759615  32.000000  1.461538  0.333333  69.333333  0.888889   \n",
       "5   2.888889  1.034946   0.666667  1.008427  0.666667  79.111111  0.444444   \n",
       "6 -21.666667  0.762195  21.555556  1.291291  0.333333  74.000000  0.666667   \n",
       "7  -8.888889  0.894040 -22.333333  0.763807  0.444444  94.555556  0.111111   \n",
       "8   7.222222  1.103503   3.111111  1.035132  0.444444  88.555556  0.666667   \n",
       "\n",
       "    h_A_mean  h_perc_hva  h_M_mean_hva  \n",
       "0  88.888889    0.778947        -21.00  \n",
       "1  79.888889    1.028846          2.40  \n",
       "2  83.666667    1.084469          6.20  \n",
       "3  69.777778    1.127820          8.50  \n",
       "4  80.888889    0.843206        -11.25  \n",
       "5  82.666667    1.205036         14.25  \n",
       "6  91.111111    0.795337        -19.75  \n",
       "7  83.888889    0.878698        -10.25  \n",
       "8  69.777778    1.265574         20.25  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scoring, columns=cols)[imp.iloc[:10,:].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>home</th>\n",
       "      <th>away</th>\n",
       "      <th>prob_avg</th>\n",
       "      <th>prob_med</th>\n",
       "      <th>prob_std</th>\n",
       "      <th>prob_max</th>\n",
       "      <th>prob_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sydney</td>\n",
       "      <td>Collingwood</td>\n",
       "      <td>0.3838922</td>\n",
       "      <td>0.38538992</td>\n",
       "      <td>0.023592498</td>\n",
       "      <td>0.42786688</td>\n",
       "      <td>0.33283982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hawthorn</td>\n",
       "      <td>Port Adelaide</td>\n",
       "      <td>0.5591566</td>\n",
       "      <td>0.56157887</td>\n",
       "      <td>0.017976481</td>\n",
       "      <td>0.59998196</td>\n",
       "      <td>0.53264475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Western Bulldogs</td>\n",
       "      <td>North Melbourne</td>\n",
       "      <td>0.78244966</td>\n",
       "      <td>0.7817625</td>\n",
       "      <td>0.024296766</td>\n",
       "      <td>0.83401483</td>\n",
       "      <td>0.73297375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelaide</td>\n",
       "      <td>West Coast</td>\n",
       "      <td>0.5862512</td>\n",
       "      <td>0.584501</td>\n",
       "      <td>0.020908002</td>\n",
       "      <td>0.6256451</td>\n",
       "      <td>0.5527714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gold Coast</td>\n",
       "      <td>Geelong</td>\n",
       "      <td>0.35618162</td>\n",
       "      <td>0.35386682</td>\n",
       "      <td>0.031124849</td>\n",
       "      <td>0.4110508</td>\n",
       "      <td>0.31608582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Richmond</td>\n",
       "      <td>Essendon</td>\n",
       "      <td>0.6260517</td>\n",
       "      <td>0.6200788</td>\n",
       "      <td>0.022261424</td>\n",
       "      <td>0.6647724</td>\n",
       "      <td>0.5977502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Greater Western Sydney</td>\n",
       "      <td>0.29331225</td>\n",
       "      <td>0.2980677</td>\n",
       "      <td>0.01953985</td>\n",
       "      <td>0.3226592</td>\n",
       "      <td>0.25272852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>St Kilda</td>\n",
       "      <td>Carlton</td>\n",
       "      <td>0.66572726</td>\n",
       "      <td>0.6639968</td>\n",
       "      <td>0.031696774</td>\n",
       "      <td>0.7432351</td>\n",
       "      <td>0.6144265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fremantle</td>\n",
       "      <td>Brisbane Lions</td>\n",
       "      <td>0.5875928</td>\n",
       "      <td>0.588754</td>\n",
       "      <td>0.024271268</td>\n",
       "      <td>0.6373708</td>\n",
       "      <td>0.5446772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               home                    away    prob_avg    prob_med  \\\n",
       "0            Sydney             Collingwood   0.3838922  0.38538992   \n",
       "1          Hawthorn           Port Adelaide   0.5591566  0.56157887   \n",
       "2  Western Bulldogs         North Melbourne  0.78244966   0.7817625   \n",
       "3          Adelaide              West Coast   0.5862512    0.584501   \n",
       "4        Gold Coast                 Geelong  0.35618162  0.35386682   \n",
       "5          Richmond                Essendon   0.6260517   0.6200788   \n",
       "6         Melbourne  Greater Western Sydney  0.29331225   0.2980677   \n",
       "7          St Kilda                 Carlton  0.66572726   0.6639968   \n",
       "8         Fremantle          Brisbane Lions   0.5875928    0.588754   \n",
       "\n",
       "      prob_std    prob_max    prob_min  \n",
       "0  0.023592498  0.42786688  0.33283982  \n",
       "1  0.017976481  0.59998196  0.53264475  \n",
       "2  0.024296766  0.83401483  0.73297375  \n",
       "3  0.020908002   0.6256451   0.5527714  \n",
       "4  0.031124849   0.4110508  0.31608582  \n",
       "5  0.022261424   0.6647724   0.5977502  \n",
       "6   0.01953985   0.3226592  0.25272852  \n",
       "7  0.031696774   0.7432351   0.6144265  \n",
       "8  0.024271268   0.6373708   0.5446772  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def modelStats(X, models=[]):\n",
    "    predictions = np.column_stack([\n",
    "        model.predict_proba(X)[:,1] for model in models\n",
    "    ])\n",
    "    return [np.mean(predictions, axis=1), np.median(predictions, axis=1), np.std(predictions, axis=1), \n",
    "            np.max(predictions, axis=1), np.min(predictions, axis=1)]\n",
    "\n",
    "arr = np.c_[ games, modelStats(scoring,best_models)[0],\n",
    "           modelStats(scoring,best_models)[1],modelStats(scoring,best_models)[2],\n",
    "            modelStats(scoring,best_models)[3], modelStats(scoring,best_models)[4]] \n",
    "pd.DataFrame(arr,columns=['home','away','prob_avg', 'prob_med', 'prob_std', 'prob_max', 'prob_min'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "for i in range(len(best_models)):\n",
    "    dump(best_models[i], 'models/model'+str(i)+'.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "best_models = []\n",
    "for i in range(15):\n",
    "    best_models.append(load('models/model'+str(i)+'.joblib'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8303385049374765\n",
      "-0.020315957750766223\n",
      "-0.912839014121544\n",
      "1.1132250915179103\n",
      "0.37881351152179543\n",
      "0.7537172258262737\n",
      "2.223278040745785\n",
      "0.6281139271832157\n",
      "2.1336644886522813\n",
      "1.341284556098238\n",
      "3.480186957595711\n",
      "0.4361284054561301\n",
      "0.7732397537936244\n",
      "0.9831722332294254\n",
      "-1.543043115148829\n",
      "3.802054339791517\n",
      "-0.3762004337292508\n",
      "0.26368217626766266\n",
      "1.6710564876840215\n",
      "2.5684328797831384\n",
      "0.9599183550303355\n",
      "1.779332797331592\n",
      "2.8283238013779446\n",
      "\n",
      "26.09556501307369\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "season_scores = []\n",
    "def afl_loss2(y_true, y_pred):\n",
    "    return np.sum(1 + np.log2(y_true * y_pred + (1 - y_true) * (1 - y_pred)))\n",
    "\n",
    "for rnd in range(1,24):\n",
    "    df = pd.DataFrame(np.c_[X_list[i],y_list[i]])\n",
    "    y_new = df[df[0] == rnd][33].values\n",
    "    x_new = df[df[0] == rnd].drop(33, axis=1).values\n",
    "    score = afl_loss2(y_new,best_models[i].predict_proba(x_new)[:,1])\n",
    "    season_scores.append(score)\n",
    "    print(score)\n",
    "print(\"\")\n",
    "print(np.sum(season_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
