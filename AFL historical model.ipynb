{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "pd.set_option(\"display.max_columns\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Proxy settings when using cntlm\n",
    "http_proxy  = \"http://localhost:3128\"\n",
    "https_proxy = \"https://localhost:3128\"\n",
    "\n",
    "proxyDict = { \n",
    "              \"http\"  : http_proxy, \n",
    "              \"https\" : https_proxy, \n",
    "            }\n",
    "proxyDict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'Richmond': 'richmond',\n",
    "    'West Coast': 'westcoast',\n",
    "    'Sydney': 'swans',\n",
    "    'Adelaide': 'adelaide',\n",
    "    'Geelong': 'geelong',\n",
    "    'Greater Western Sydney': 'gws',\n",
    "    'Melbourne': 'melbourne',\n",
    "    'Port Adelaide': 'padelaide',\n",
    "    'Collingwood': 'collingwood',\n",
    "    'Hawthorn': 'hawthorn',\n",
    "    'Essendon': 'essendon',\n",
    "    'Western Bulldogs': 'bullldogs',\n",
    "    'St Kilda': 'stkilda',\n",
    "    'North Melbourne': 'kangaroos',\n",
    "    'Kangaroos' : 'kangaroos',\n",
    "    'Fremantle': 'fremantle',\n",
    "    'Brisbane Lions': 'brisbanel',\n",
    "    'Gold Coast': 'goldcoast',\n",
    "    'Carlton': 'carlton'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_prep.team_history import History\n",
    "team_df = History(mapping, proxyDict).generate_team_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "History(mapping, proxyDict).generate_game_data('training-all/', team_df)\n",
    "History(mapping, proxyDict).generate_game_data_ha('training-hva/', team_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2789, 33)\n",
      "(2789,)\n",
      "Wins vs losses 0.577626389386877\n"
     ]
    }
   ],
   "source": [
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for i in range(1,16):\n",
    "    X = np.concatenate([\n",
    "        np.load('training-all/training-'+str(2019-i)+'.npy'),\n",
    "        np.load('training-hva/training-'+str(2019-i)+'.npy')[:,1:] #remove the rnd column\n",
    "                ], axis=1)\n",
    "    mask = np.isnan(X).any(axis=1)\n",
    "    index = np.where(mask==True)[0][0] ## X8 has a row containing nulls\n",
    "    X = np.delete(X, index, 0)\n",
    "    X_list.append(X)\n",
    "    \n",
    "    y = np.load('training-all/results-'+str(2019-i)+'.npy')\n",
    "    y = np.delete(y, index, 0)\n",
    "    y_list.append(y)\n",
    "    \n",
    "X = np.concatenate(X_list, axis=0)\n",
    "y = np.concatenate(y_list, axis=0)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print('Wins vs losses',np.sum(y)/float(len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rnd</th>\n",
       "      <th>h_F_mean</th>\n",
       "      <th>h_F_std</th>\n",
       "      <th>h_A_mean</th>\n",
       "      <th>h_A_std</th>\n",
       "      <th>h_M_mean</th>\n",
       "      <th>h_M_std</th>\n",
       "      <th>h_W_sum</th>\n",
       "      <th>h_perc</th>\n",
       "      <th>a_F_mean</th>\n",
       "      <th>a_F_std</th>\n",
       "      <th>a_A_mean</th>\n",
       "      <th>a_A_std</th>\n",
       "      <th>a_M_mean</th>\n",
       "      <th>a_M_std</th>\n",
       "      <th>a_W_sum</th>\n",
       "      <th>a_perc</th>\n",
       "      <th>h_F_mean_hva</th>\n",
       "      <th>h_F_std_hva</th>\n",
       "      <th>h_A_mean_hva</th>\n",
       "      <th>h_A_std_hva</th>\n",
       "      <th>h_M_mean_hva</th>\n",
       "      <th>h_M_std_hva</th>\n",
       "      <th>h_W_sum_hva</th>\n",
       "      <th>h_perc_hva</th>\n",
       "      <th>a_F_mean_hva</th>\n",
       "      <th>a_F_std_hva</th>\n",
       "      <th>a_A_mean_hva</th>\n",
       "      <th>a_A_std_hva</th>\n",
       "      <th>a_M_mean_hva</th>\n",
       "      <th>a_M_std_hva</th>\n",
       "      <th>a_W_sum_hva</th>\n",
       "      <th>a_perc_hva</th>\n",
       "      <th>F_mean</th>\n",
       "      <th>F_std</th>\n",
       "      <th>A_mean</th>\n",
       "      <th>A_std</th>\n",
       "      <th>M_mean</th>\n",
       "      <th>M_std</th>\n",
       "      <th>W_sum</th>\n",
       "      <th>perc</th>\n",
       "      <th>F_mean_hva</th>\n",
       "      <th>F_std_hva</th>\n",
       "      <th>A_mean_hva</th>\n",
       "      <th>A_std_hva</th>\n",
       "      <th>M_mean_hva</th>\n",
       "      <th>M_std_hva</th>\n",
       "      <th>W_sum_hva</th>\n",
       "      <th>perc_hva</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>101.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>106.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953052</td>\n",
       "      <td>109.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.190217</td>\n",
       "      <td>121.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.273684</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.008547</td>\n",
       "      <td>0.926941</td>\n",
       "      <td>1.025424</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.157609</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800737</td>\n",
       "      <td>1.262890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>101.666667</td>\n",
       "      <td>27.577164</td>\n",
       "      <td>100.666667</td>\n",
       "      <td>16.263456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.263456</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.009934</td>\n",
       "      <td>82.666667</td>\n",
       "      <td>5.656854</td>\n",
       "      <td>101.333333</td>\n",
       "      <td>4.949747</td>\n",
       "      <td>-18.666667</td>\n",
       "      <td>4.949747</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>111.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.211957</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>1.229839</td>\n",
       "      <td>1.281609</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993421</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>3.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.053571</td>\n",
       "      <td>-1.300000</td>\n",
       "      <td>3.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.237983</td>\n",
       "      <td>1.420915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>14.275854</td>\n",
       "      <td>74.166667</td>\n",
       "      <td>39.274674</td>\n",
       "      <td>30.833333</td>\n",
       "      <td>39.274674</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.415730</td>\n",
       "      <td>83.666667</td>\n",
       "      <td>26.780590</td>\n",
       "      <td>82.166667</td>\n",
       "      <td>21.335417</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>21.335417</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.018256</td>\n",
       "      <td>111.00</td>\n",
       "      <td>13.435029</td>\n",
       "      <td>67.0</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>44.00</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.656716</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>25.455844</td>\n",
       "      <td>86.666667</td>\n",
       "      <td>29.698485</td>\n",
       "      <td>-17.666667</td>\n",
       "      <td>29.698485</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.796154</td>\n",
       "      <td>1.254980</td>\n",
       "      <td>1.608696</td>\n",
       "      <td>0.533067</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.902637</td>\n",
       "      <td>0.773077</td>\n",
       "      <td>1.840821</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>20.555556</td>\n",
       "      <td>-2.490566</td>\n",
       "      <td>1.840821</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.390349</td>\n",
       "      <td>2.080900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>100.555556</td>\n",
       "      <td>14.169888</td>\n",
       "      <td>75.555556</td>\n",
       "      <td>33.070272</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>33.070272</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.330882</td>\n",
       "      <td>65.111111</td>\n",
       "      <td>19.455076</td>\n",
       "      <td>93.444444</td>\n",
       "      <td>11.624328</td>\n",
       "      <td>-28.333333</td>\n",
       "      <td>11.624328</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.696790</td>\n",
       "      <td>110.75</td>\n",
       "      <td>9.539392</td>\n",
       "      <td>58.5</td>\n",
       "      <td>43.405069</td>\n",
       "      <td>52.25</td>\n",
       "      <td>43.405069</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.893162</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>7.023769</td>\n",
       "      <td>-41.000000</td>\n",
       "      <td>7.023769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.563830</td>\n",
       "      <td>1.544369</td>\n",
       "      <td>2.089623</td>\n",
       "      <td>0.728339</td>\n",
       "      <td>1.362770</td>\n",
       "      <td>0.808561</td>\n",
       "      <td>0.622340</td>\n",
       "      <td>2.844919</td>\n",
       "      <td>6.179740</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-1.274390</td>\n",
       "      <td>2.844919</td>\n",
       "      <td>6.179740</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.910021</td>\n",
       "      <td>3.357684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.0</td>\n",
       "      <td>97.307692</td>\n",
       "      <td>18.372410</td>\n",
       "      <td>72.076923</td>\n",
       "      <td>32.980251</td>\n",
       "      <td>25.230769</td>\n",
       "      <td>32.980251</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.350053</td>\n",
       "      <td>88.307692</td>\n",
       "      <td>19.942341</td>\n",
       "      <td>70.230769</td>\n",
       "      <td>15.023214</td>\n",
       "      <td>18.076923</td>\n",
       "      <td>15.023214</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.257393</td>\n",
       "      <td>109.60</td>\n",
       "      <td>7.804913</td>\n",
       "      <td>62.2</td>\n",
       "      <td>39.306488</td>\n",
       "      <td>47.40</td>\n",
       "      <td>39.306488</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.762058</td>\n",
       "      <td>93.166667</td>\n",
       "      <td>17.672012</td>\n",
       "      <td>68.166667</td>\n",
       "      <td>13.935566</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>13.935566</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.366748</td>\n",
       "      <td>1.101916</td>\n",
       "      <td>1.176386</td>\n",
       "      <td>0.921276</td>\n",
       "      <td>0.441654</td>\n",
       "      <td>1.026287</td>\n",
       "      <td>0.912469</td>\n",
       "      <td>2.195286</td>\n",
       "      <td>2.820588</td>\n",
       "      <td>1.395745</td>\n",
       "      <td>1.896000</td>\n",
       "      <td>2.195286</td>\n",
       "      <td>2.820588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.073692</td>\n",
       "      <td>1.289234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rnd    h_F_mean    h_F_std    h_A_mean    h_A_std   h_M_mean    h_M_std  \\\n",
       "0   3.0  101.500000   0.000000  106.500000   0.000000  -5.000000   0.000000   \n",
       "1   4.0  101.666667  27.577164  100.666667  16.263456   1.000000  16.263456   \n",
       "2   7.0  105.000000  14.275854   74.166667  39.274674  30.833333  39.274674   \n",
       "3  10.0  100.555556  14.169888   75.555556  33.070272  25.000000  33.070272   \n",
       "4  15.0   97.307692  18.372410   72.076923  32.980251  25.230769  32.980251   \n",
       "\n",
       "   h_W_sum    h_perc    a_F_mean    a_F_std    a_A_mean    a_A_std   a_M_mean  \\\n",
       "0      1.0  0.953052  109.500000   0.000000   92.000000   0.000000  17.500000   \n",
       "1      2.0  1.009934   82.666667   5.656854  101.333333   4.949747 -18.666667   \n",
       "2      5.0  1.415730   83.666667  26.780590   82.166667  21.335417   1.500000   \n",
       "3      7.0  1.330882   65.111111  19.455076   93.444444  11.624328 -28.333333   \n",
       "4     10.0  1.350053   88.307692  19.942341   70.230769  15.023214  18.076923   \n",
       "\n",
       "     a_M_std  a_W_sum    a_perc  h_F_mean_hva  h_F_std_hva  h_A_mean_hva  \\\n",
       "0   0.000000      2.0  1.190217        121.00     0.000000          95.0   \n",
       "1   4.949747      0.0  0.815789        111.50     0.000000          92.0   \n",
       "2  21.335417      3.0  1.018256        111.00    13.435029          67.0   \n",
       "3  11.624328      2.0  0.696790        110.75     9.539392          58.5   \n",
       "4  15.023214     10.0  1.257393        109.60     7.804913          62.2   \n",
       "\n",
       "   h_A_std_hva  h_M_mean_hva  h_M_std_hva  h_W_sum_hva  h_perc_hva  \\\n",
       "0     0.000000         26.00     0.000000          0.0    1.273684   \n",
       "1     0.000000         19.50     0.000000          2.0    1.211957   \n",
       "2     4.242641         44.00     4.242641          3.0    1.656716   \n",
       "3    43.405069         52.25    43.405069          4.0    1.893162   \n",
       "4    39.306488         47.40    39.306488          5.0    1.762058   \n",
       "\n",
       "   a_F_mean_hva  a_F_std_hva  a_A_mean_hva  a_A_std_hva  a_M_mean_hva  \\\n",
       "0    118.000000     0.000000    117.000000     0.000000      1.000000   \n",
       "1     87.000000     0.000000    102.000000     0.000000    -15.000000   \n",
       "2     69.000000    25.455844     86.666667    29.698485    -17.666667   \n",
       "3     53.000000     7.000000     94.000000     7.023769    -41.000000   \n",
       "4     93.166667    17.672012     68.166667    13.935566     25.000000   \n",
       "\n",
       "   a_M_std_hva  a_W_sum_hva  a_perc_hva    F_mean     F_std    A_mean  \\\n",
       "0     0.000000          0.0    1.008547  0.926941  1.025424  0.000000   \n",
       "1     0.000000          0.0    0.852941  1.229839  1.281609  4.875000   \n",
       "2    29.698485          1.0    0.796154  1.254980  1.608696  0.533067   \n",
       "3     7.023769          0.0    0.563830  1.544369  2.089623  0.728339   \n",
       "4    13.935566          6.0    1.366748  1.101916  1.176386  0.921276   \n",
       "\n",
       "      A_std    M_mean     M_std     W_sum      perc  F_mean_hva  F_std_hva  \\\n",
       "0  0.000000  1.157609  0.811966  0.000000  0.000000   -0.285714  26.000000   \n",
       "1  0.000000  0.993421  0.901961  3.285714  0.000000   -0.053571  -1.300000   \n",
       "2  0.527778  0.902637  0.773077  1.840821  0.142857   20.555556  -2.490566   \n",
       "3  1.362770  0.808561  0.622340  2.844919  6.179740   -0.882353  -1.274390   \n",
       "4  0.441654  1.026287  0.912469  2.195286  2.820588    1.395745   1.896000   \n",
       "\n",
       "   A_mean_hva  A_std_hva  M_mean_hva  M_std_hva  W_sum_hva  perc_hva  \n",
       "0    0.000000   0.000000    0.500000   0.000000   0.800737  1.262890  \n",
       "1    3.285714   0.000000    0.000000   0.000000   1.237983  1.420915  \n",
       "2    1.840821   0.142857    1.666667   3.000000   1.390349  2.080900  \n",
       "3    2.844919   6.179740    3.500000   0.000000   1.910021  3.357684  \n",
       "4    2.195286   2.820588    1.000000   0.833333   1.073692  1.289234  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_prep import feature_eng\n",
    "training_cols = feature_eng.Features().training_cols()\n",
    "pd.DataFrame(Features().div_cols(X_list[0]), columns=training_cols).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "def afl_loss(y_true,y_pred): \n",
    "    return -np.sum(1+np.log2(y_true*y_pred + (1-y_true)*(1-y_pred)))\n",
    "scorer = make_scorer(afl_loss, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupting!\n",
      "Season 2018\n",
      "Bayes CV search took 86.95 seconds for 100 candidates parameter settings.\n",
      "val. score: 66.21462718977243\n",
      "test score: 24.554828510408274\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=0.22952905282604238, gamma=0,\n",
      "       learning_rate=0.008024991939619319, max_delta_step=0, max_depth=4,\n",
      "       min_child_weight=1, missing=None, n_estimators=320, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=1.0, reg_lambda=0.14994296215562034, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=0.1)\n",
      "\n",
      "Interrupting!\n",
      "Season 2017\n",
      "Bayes CV search took 4.72 seconds for 100 candidates parameter settings.\n",
      "val. score: 66.20446543412856\n",
      "test score: 12.118672041501942\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=0.554930552502681, gamma=0,\n",
      "       learning_rate=0.0066436041662496104, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=369, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.8595178844913383, reg_lambda=0.273012910775227,\n",
      "       scale_pos_weight=1, seed=None, silent=True,\n",
      "       subsample=0.9732623416547129)\n",
      "\n",
      "Season 2016\n",
      "Bayes CV search took 389.54 seconds for 100 candidates parameter settings.\n",
      "val. score: 62.328652528395104\n",
      "test score: 41.20116034736016\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=0.1863764596220343, gamma=0,\n",
      "       learning_rate=0.010804394635734946, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=287, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.0, reg_lambda=0.0, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.5470898466825782)\n",
      "\n",
      "Interrupting!\n",
      "Season 2015\n",
      "Bayes CV search took 8.48 seconds for 100 candidates parameter settings.\n",
      "val. score: 65.90954104971364\n",
      "test score: 21.9033623669582\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=0.12509443759800748, gamma=0,\n",
      "       learning_rate=0.006180037144391408, max_delta_step=0, max_depth=4,\n",
      "       min_child_weight=1, missing=None, n_estimators=483, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.6771859049881243, reg_lambda=0.5567438816250553,\n",
      "       scale_pos_weight=1, seed=None, silent=True,\n",
      "       subsample=0.45808625557399263)\n",
      "\n",
      "Interrupting!\n",
      "Season 2014\n",
      "Bayes CV search took 21.58 seconds for 100 candidates parameter settings.\n",
      "val. score: 65.27893204483078\n",
      "test score: 21.24866425575254\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=0.17915728787923182, gamma=0,\n",
      "       learning_rate=0.0052945165765379545, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=369, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.19162822639303215, reg_lambda=0.7732186340494913,\n",
      "       scale_pos_weight=1, seed=None, silent=True,\n",
      "       subsample=0.517402974347346)\n",
      "\n",
      "Interrupting!\n",
      "Season 2013\n",
      "Bayes CV search took 114.27 seconds for 100 candidates parameter settings.\n",
      "val. score: 65.26448312888263\n",
      "test score: 30.44737331428933\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=0.299220094439118, gamma=0,\n",
      "       learning_rate=0.007431213787338181, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=369, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.0, reg_lambda=0.8299008883844465, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=0.10136890133213892)\n",
      "\n",
      "Season 2012\n",
      "Bayes CV search took 427.56 seconds for 100 candidates parameter settings.\n",
      "val. score: 62.3219042258288\n",
      "test score: 35.65546446834596\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=0.31838808957324355, gamma=0,\n",
      "       learning_rate=0.0049118281751529234, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=500, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=1.0, reg_lambda=0.0, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.1)\n",
      "\n",
      "Season 2011\n",
      "Bayes CV search took 431.07 seconds for 100 candidates parameter settings.\n",
      "val. score: 61.255469845265125\n",
      "test score: 38.8466265425877\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=0.3406264526753112, gamma=0,\n",
      "       learning_rate=0.004772807142214197, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=483, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.0, reg_lambda=0.2888110193521379, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=0.1)\n",
      "\n",
      "Interrupting!\n",
      "Season 2010\n",
      "Bayes CV search took 41.80 seconds for 100 candidates parameter settings.\n",
      "val. score: 65.85661445392618\n",
      "test score: 17.510310724331887\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=0.35618471512599825, gamma=0,\n",
      "       learning_rate=0.006477134049046994, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=417, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.4791469965156055, reg_lambda=0.6573698490167822,\n",
      "       scale_pos_weight=1, seed=None, silent=True,\n",
      "       subsample=0.9112630425672913)\n",
      "\n",
      "Interrupting!\n",
      "Season 2009\n",
      "Bayes CV search took 92.72 seconds for 100 candidates parameter settings.\n",
      "val. score: 65.74093411831035\n",
      "test score: 24.592251821492617\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=0.1, gamma=0,\n",
      "       learning_rate=0.006830623028615203, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=500, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.3501840702892103, reg_lambda=0.0, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=0.8814723749917811)\n",
      "\n",
      "Interrupting!\n",
      "Season 2008\n",
      "Bayes CV search took 56.85 seconds for 100 candidates parameter settings.\n",
      "val. score: 65.20437001769831\n",
      "test score: 24.56744682412761\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=0.16876380732187968, gamma=0,\n",
      "       learning_rate=0.02911995140005176, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=101, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=1.0, reg_lambda=1.0, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.4890578398270503)\n",
      "\n",
      "Interrupting!\n",
      "Season 2007\n",
      "Bayes CV search took 17.38 seconds for 100 candidates parameter settings.\n",
      "val. score: 66.12533539160744\n",
      "test score: 16.577499275929334\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=0.28697245985283415, gamma=0,\n",
      "       learning_rate=0.013704640456400042, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=267, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.15352669566463586, reg_lambda=0.03779613754327816,\n",
      "       scale_pos_weight=1, seed=None, silent=True,\n",
      "       subsample=0.37253898340707947)\n",
      "\n",
      "Interrupting!\n",
      "Season 2006\n",
      "Bayes CV search took 46.34 seconds for 100 candidates parameter settings.\n",
      "val. score: 67.89823981425262\n",
      "test score: 10.383166856929275\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=0.14333835274136597, gamma=0,\n",
      "       learning_rate=0.011675823386626939, max_delta_step=0, max_depth=4,\n",
      "       min_child_weight=1, missing=None, n_estimators=248, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.9022353442512775, reg_lambda=0.9507923215793014,\n",
      "       scale_pos_weight=1, seed=None, silent=True,\n",
      "       subsample=0.8915024531120394)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupting!\n",
      "Season 2005\n",
      "Bayes CV search took 11.31 seconds for 100 candidates parameter settings.\n",
      "val. score: 66.85986509480585\n",
      "test score: 16.186483059862564\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=0.39806024904997983, gamma=0,\n",
      "       learning_rate=0.007599846049523707, max_delta_step=0, max_depth=4,\n",
      "       min_child_weight=1, missing=None, n_estimators=259, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.1638335295329413, reg_lambda=0.5390787182339127,\n",
      "       scale_pos_weight=1, seed=None, silent=True,\n",
      "       subsample=0.6030323927708772)\n",
      "\n",
      "Interrupting!\n",
      "Season 2004\n",
      "Bayes CV search took 13.07 seconds for 100 candidates parameter settings.\n",
      "val. score: 65.35277653745631\n",
      "test score: 18.890485117127657\n",
      "XGBClassifier(base_score=0.57574568288854, booster='gbtree',\n",
      "       colsample_bylevel=1, colsample_bytree=0.30559491913649683, gamma=0,\n",
      "       learning_rate=0.006590167875338094, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=490, n_jobs=-1,\n",
      "       nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.08197124974752691, reg_lambda=0.7318538240678378,\n",
      "       scale_pos_weight=1, seed=None, silent=True,\n",
      "       subsample=0.2934778894313663)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from data_prep.feature_eng import Features\n",
    "from time import time\n",
    "scores = []\n",
    "best_models = []\n",
    "bayesian_df = pd.DataFrame()\n",
    "\n",
    "space={\n",
    "        'n_estimators': Integer(100, 500),\n",
    "        'max_depth': Integer(3, 5),\n",
    "        'learning_rate': Real(10**-3, 0.1, \"log-uniform\"),\n",
    "        'colsample_bytree': Real(0.1, 1.0, \"uniform\"),\n",
    "        'subsample': Real(0.1, 1.0, \"uniform\"),\n",
    "        'reg_lambda': Real(0.0, 1.0, \"uniform\"),\n",
    "        'reg_alpha': Real(0.0, 1.0, \"uniform\"),\n",
    "}\n",
    "\n",
    "n_calls=100\n",
    "cutoff_score=65\n",
    "\n",
    "for j in range(len(X_list)):\n",
    "    classifier = XGBClassifier(base_score=0.57574568288854, n_jobs=-1)\n",
    "    y = y_list.copy()\n",
    "    X = X_list.copy()\n",
    "    y_test = y.pop(j)\n",
    "    X_test = X.pop(j)\n",
    "    y_train = np.concatenate(y, axis=0)\n",
    "    X_train = np.concatenate(X, axis=0)\n",
    "\n",
    "    X_train = Features().div_cols(X_train)\n",
    "    X_test = Features().div_cols(X_test)\n",
    "\n",
    "    start = time()\n",
    "    opt = BayesSearchCV(classifier, search_spaces=space, scoring=scorer, cv=5, n_iter=n_calls, n_jobs=-1)\n",
    "    # callback handler\n",
    "    def on_step(iteration):\n",
    "        score = opt.best_score_\n",
    "        if score > cutoff_score:\n",
    "            print('Interrupting!')\n",
    "            return True\n",
    "\n",
    "    opt.fit(X_train, y_train, callback=on_step)\n",
    "    model = opt.best_estimator_ \n",
    "    print('Season',2018-j)\n",
    "    print(\"Bayes CV search took %.2f seconds for %d candidates\"\n",
    "          \" parameter settings.\" % ((time() - start), n_calls))\n",
    "    print(\"val. score:\", opt.best_score_)\n",
    "    print(\"test score:\", opt.score(X_test, y_test))\n",
    "    print(model)\n",
    "    print(\"\")\n",
    "    best_models.append(model)\n",
    "    scores.append(opt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>W_sum_hva</th>\n",
       "      <td>0.055276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perc_hva</th>\n",
       "      <td>0.050112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M_mean_hva</th>\n",
       "      <td>0.042506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M_mean</th>\n",
       "      <td>0.040665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F_mean</th>\n",
       "      <td>0.032111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M_std</th>\n",
       "      <td>0.030897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h_M_mean</th>\n",
       "      <td>0.029852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h_perc</th>\n",
       "      <td>0.025927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_perc</th>\n",
       "      <td>0.025618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_M_mean</th>\n",
       "      <td>0.025166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            importance\n",
       "W_sum_hva     0.055276\n",
       "perc_hva      0.050112\n",
       "M_mean_hva    0.042506\n",
       "M_mean        0.040665\n",
       "F_mean        0.032111\n",
       "M_std         0.030897\n",
       "h_M_mean      0.029852\n",
       "h_perc        0.025927\n",
       "a_perc        0.025618\n",
       "a_M_mean      0.025166"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def averagingImp(models=[]):\n",
    "    predictions = np.column_stack([\n",
    "        model.feature_importances_ for model in models\n",
    "    ])\n",
    "    return np.mean(predictions, axis=1)\n",
    "\n",
    "imp = pd.DataFrame(data=averagingImp(best_models),\n",
    "             index=training_cols, \n",
    "             columns=['importance']).sort_values(by=['importance'], ascending=False)\n",
    "imp.iloc[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averagingModels(X, models=[]):\n",
    "    predictions = np.column_stack([\n",
    "        model.predict_proba(X)[:,1] for model in models\n",
    "    ])\n",
    "    return np.mean(predictions, axis=1)\n",
    "\n",
    "def stDevModels(X, models=[]):\n",
    "    predictions = np.column_stack([\n",
    "        model.predict_proba(X)[:,1] for model in models\n",
    "    ])\n",
    "    return np.std(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x128d75f60>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12.5,4))\n",
    "line_height = 2\n",
    "data = averagingModels(X_train,best_models)\n",
    "x = plt.hist(data, bins='auto', density=True, facecolor='green', alpha=0.5)\n",
    "\n",
    "plt.vlines(np.percentile(data, 5), 0, line_height, linestyle=\"--\", colors='green', label=\"5%\")\n",
    "plt.vlines(np.percentile(data, 50), 0, line_height, linestyle=\"--\", colors='red', label=\"50%\")\n",
    "plt.vlines(np.percentile(data, 95), 0, line_height, linestyle=\"--\", colors='blue', label=\"95%\")\n",
    "plt.xlabel('Probability')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_score</th>\n",
       "      <th>booster</th>\n",
       "      <th>colsample_bylevel</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>gamma</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_delta_step</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>missing</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>n_jobs</th>\n",
       "      <th>nthread</th>\n",
       "      <th>objective</th>\n",
       "      <th>random_state</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>scale_pos_weight</th>\n",
       "      <th>seed</th>\n",
       "      <th>silent</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>0.229529</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008025</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.149943</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>0.554931</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006644</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>369</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.859518</td>\n",
       "      <td>0.273013</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.973262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>0.186376</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010804</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>287</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.547090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125094</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006180</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>483</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.677186</td>\n",
       "      <td>0.556744</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.458086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>0.179157</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005295</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>369</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.191628</td>\n",
       "      <td>0.773219</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.517403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>0.299220</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007431</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>369</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.829901</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.101369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>0.318388</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004912</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>0.340626</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004773</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>483</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.288811</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>0.356185</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006477</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>417</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.479147</td>\n",
       "      <td>0.657370</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.911263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006831</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.350184</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.881472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>0.168764</td>\n",
       "      <td>0</td>\n",
       "      <td>0.029120</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.489058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>0.286972</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013705</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>267</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.153527</td>\n",
       "      <td>0.037796</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.372539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>0.143338</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011676</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>248</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.902235</td>\n",
       "      <td>0.950792</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.891502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>0.398060</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>259</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.163834</td>\n",
       "      <td>0.539079</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.603032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.575746</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>0.305595</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006590</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>490</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.081971</td>\n",
       "      <td>0.731854</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>0.293478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    base_score booster  colsample_bylevel  colsample_bytree  gamma  \\\n",
       "0     0.575746  gbtree                  1          0.229529      0   \n",
       "1     0.575746  gbtree                  1          0.554931      0   \n",
       "2     0.575746  gbtree                  1          0.186376      0   \n",
       "3     0.575746  gbtree                  1          0.125094      0   \n",
       "4     0.575746  gbtree                  1          0.179157      0   \n",
       "5     0.575746  gbtree                  1          0.299220      0   \n",
       "6     0.575746  gbtree                  1          0.318388      0   \n",
       "7     0.575746  gbtree                  1          0.340626      0   \n",
       "8     0.575746  gbtree                  1          0.356185      0   \n",
       "9     0.575746  gbtree                  1          0.100000      0   \n",
       "10    0.575746  gbtree                  1          0.168764      0   \n",
       "11    0.575746  gbtree                  1          0.286972      0   \n",
       "12    0.575746  gbtree                  1          0.143338      0   \n",
       "13    0.575746  gbtree                  1          0.398060      0   \n",
       "14    0.575746  gbtree                  1          0.305595      0   \n",
       "\n",
       "    learning_rate  max_delta_step  max_depth  min_child_weight  missing  \\\n",
       "0        0.008025               0          4                 1      NaN   \n",
       "1        0.006644               0          3                 1      NaN   \n",
       "2        0.010804               0          3                 1      NaN   \n",
       "3        0.006180               0          4                 1      NaN   \n",
       "4        0.005295               0          3                 1      NaN   \n",
       "5        0.007431               0          3                 1      NaN   \n",
       "6        0.004912               0          3                 1      NaN   \n",
       "7        0.004773               0          3                 1      NaN   \n",
       "8        0.006477               0          3                 1      NaN   \n",
       "9        0.006831               0          3                 1      NaN   \n",
       "10       0.029120               0          3                 1      NaN   \n",
       "11       0.013705               0          3                 1      NaN   \n",
       "12       0.011676               0          4                 1      NaN   \n",
       "13       0.007600               0          4                 1      NaN   \n",
       "14       0.006590               0          5                 1      NaN   \n",
       "\n",
       "    n_estimators  n_jobs nthread        objective  random_state  reg_alpha  \\\n",
       "0            320      -1    None  binary:logistic             0   1.000000   \n",
       "1            369      -1    None  binary:logistic             0   0.859518   \n",
       "2            287      -1    None  binary:logistic             0   0.000000   \n",
       "3            483      -1    None  binary:logistic             0   0.677186   \n",
       "4            369      -1    None  binary:logistic             0   0.191628   \n",
       "5            369      -1    None  binary:logistic             0   0.000000   \n",
       "6            500      -1    None  binary:logistic             0   1.000000   \n",
       "7            483      -1    None  binary:logistic             0   0.000000   \n",
       "8            417      -1    None  binary:logistic             0   0.479147   \n",
       "9            500      -1    None  binary:logistic             0   0.350184   \n",
       "10           101      -1    None  binary:logistic             0   1.000000   \n",
       "11           267      -1    None  binary:logistic             0   0.153527   \n",
       "12           248      -1    None  binary:logistic             0   0.902235   \n",
       "13           259      -1    None  binary:logistic             0   0.163834   \n",
       "14           490      -1    None  binary:logistic             0   0.081971   \n",
       "\n",
       "    reg_lambda  scale_pos_weight  seed  silent  subsample  \n",
       "0     0.149943                 1  None    True   0.100000  \n",
       "1     0.273013                 1  None    True   0.973262  \n",
       "2     0.000000                 1  None    True   0.547090  \n",
       "3     0.556744                 1  None    True   0.458086  \n",
       "4     0.773219                 1  None    True   0.517403  \n",
       "5     0.829901                 1  None    True   0.101369  \n",
       "6     0.000000                 1  None    True   0.100000  \n",
       "7     0.288811                 1  None    True   0.100000  \n",
       "8     0.657370                 1  None    True   0.911263  \n",
       "9     0.000000                 1  None    True   0.881472  \n",
       "10    1.000000                 1  None    True   0.489058  \n",
       "11    0.037796                 1  None    True   0.372539  \n",
       "12    0.950792                 1  None    True   0.891502  \n",
       "13    0.539079                 1  None    True   0.603032  \n",
       "14    0.731854                 1  None    True   0.293478  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for model in best_models:\n",
    "    rows.append(model.get_params())\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/d719674/dev/code/AFL-Monash-comp/data_prep/web_scraping.py:35: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 35 of the file /Users/d719674/dev/code/AFL-Monash-comp/data_prep/web_scraping.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  r = requests.get(url, headers={'User-Agent': 'test'}, proxies=self.proxy)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['West Coast', 'Melbourne'],\n",
       " ['Collingwood', 'St Kilda'],\n",
       " ['Brisbane Lions', 'Adelaide'],\n",
       " ['Geelong', 'Western Bulldogs'],\n",
       " ['Essendon', 'Fremantle'],\n",
       " ['North Melbourne', 'Sydney'],\n",
       " ['Port Adelaide', 'Gold Coast'],\n",
       " ['Richmond', 'Hawthorn'],\n",
       " ['Greater Western Sydney', 'Carlton']]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_prep.web_scraping import Scrape\n",
    "\n",
    "games = []\n",
    "start = 98\n",
    "for i in range(start,start+9):\n",
    "    games.append(Scrape(mapping, proxyDict).scrape_game(i))\n",
    "games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/d719674/dev/code/AFL-Monash-comp/data_prep/web_scraping.py:16: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 16 of the file /Users/d719674/dev/code/AFL-Monash-comp/data_prep/web_scraping.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(r.text, \"html\")\n"
     ]
    }
   ],
   "source": [
    "from data_prep.scoring import Scoring\n",
    "scoring = Scoring(mapping, proxyDict).score_data(games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_prep.feature_eng import Features\n",
    "scoring_enr = Features().div_cols(scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>home</th>\n",
       "      <th>away</th>\n",
       "      <th>prob_avg</th>\n",
       "      <th>prob_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>West Coast</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>0.77801985</td>\n",
       "      <td>0.024330983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Collingwood</td>\n",
       "      <td>St Kilda</td>\n",
       "      <td>0.6935017</td>\n",
       "      <td>0.026173431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brisbane Lions</td>\n",
       "      <td>Adelaide</td>\n",
       "      <td>0.5097246</td>\n",
       "      <td>0.016936135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Geelong</td>\n",
       "      <td>Western Bulldogs</td>\n",
       "      <td>0.71039027</td>\n",
       "      <td>0.017297562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Essendon</td>\n",
       "      <td>Fremantle</td>\n",
       "      <td>0.5424909</td>\n",
       "      <td>0.012114255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>North Melbourne</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>0.50362676</td>\n",
       "      <td>0.012269485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Port Adelaide</td>\n",
       "      <td>Gold Coast</td>\n",
       "      <td>0.6798676</td>\n",
       "      <td>0.021793908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Richmond</td>\n",
       "      <td>Hawthorn</td>\n",
       "      <td>0.5984039</td>\n",
       "      <td>0.021347562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Greater Western Sydney</td>\n",
       "      <td>Carlton</td>\n",
       "      <td>0.79436654</td>\n",
       "      <td>0.0129788965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     home              away    prob_avg      prob_std\n",
       "0              West Coast         Melbourne  0.77801985   0.024330983\n",
       "1             Collingwood          St Kilda   0.6935017   0.026173431\n",
       "2          Brisbane Lions          Adelaide   0.5097246   0.016936135\n",
       "3                 Geelong  Western Bulldogs  0.71039027   0.017297562\n",
       "4                Essendon         Fremantle   0.5424909   0.012114255\n",
       "5         North Melbourne            Sydney  0.50362676   0.012269485\n",
       "6           Port Adelaide        Gold Coast   0.6798676   0.021793908\n",
       "7                Richmond          Hawthorn   0.5984039   0.021347562\n",
       "8  Greater Western Sydney           Carlton  0.79436654  0.0129788965"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.c_[ games, averagingModels(scoring_enr,best_models), stDevModels(scoring_enr,best_models) ] \n",
    "pd.DataFrame(arr,columns=['home','away','prob_avg', 'prob_std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "for i in range(len(best_models)):\n",
    "    dump(best_models[i], 'models/model'+str(i)+'.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "best_models = []\n",
    "for i in range(15):\n",
    "    best_models.append(load('models/model'+str(i)+'.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
